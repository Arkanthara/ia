{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données\n",
    "Les données à utiliser pour ce TP se trouvent dans les fichiers data.csv et data_test.csv.\n",
    "Les 5 premières colonnes spécifient les variables indépendantes tandis que la dernière colonne correspond à la variable dépendante (label).\n",
    "Le fichier data_test.csv sert à évaluer les performances des arbres développés à partir des données du fichier data.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai défini la fonction suivante afin de convertir un fichier csv en np.array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def convert_csv2array(name: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert a csv file into numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: string\n",
    "        Path to the file\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    header and data of csv\n",
    "    \"\"\"\n",
    "    file = open(name, 'r')\n",
    "    data = []\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "    data = np.array(data)\n",
    "    return data[0, :], data[1:, :].astype(int)\n",
    "\n",
    "header, data = convert_csv2array('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1: Entropie et gain d’information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculer l’entropie de la variable dépendante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the independant variable: 0.9738003694382252\n"
     ]
    }
   ],
   "source": [
    "def entropy(data: np.ndarray, column: int) -> float:\n",
    "    \"\"\"\n",
    "    Give the entropy of the given column of data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data\n",
    "    column: int\n",
    "        column on which we want to compute entropy\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    The entropy of the given column of data \n",
    "    \"\"\"\n",
    "    values, count = np.unique(data[:, column], return_counts=True)\n",
    "    result = 0\n",
    "    for i in range(len(values)):\n",
    "        nb = count[i] / len(data)\n",
    "        result += -nb * np.log2(nb)\n",
    "    return result\n",
    "\n",
    "print(f\"Entropy of the independant variable: {entropy(data, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculer le gain d’information réalisé après l’application de trois critères de décision aléatoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the criteria B, the gain of information is: 0.1520457522567905\n",
      "For the criteria D, the gain of information is: 0.16681614778733878\n",
      "For the criteria A, the gain of information is: 0.02894167191284036\n"
     ]
    }
   ],
   "source": [
    "def conditionnal_entropy(data: np.ndarray, column_1: int, column_2: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute conditionnal entropy H(X|Y)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data used to compute conditionnal entropy\n",
    "    column_1: int\n",
    "        Column of X\n",
    "    column_2: int\n",
    "        Column of Y\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return the result of H(X|Y)\n",
    "    \"\"\"\n",
    "    values, count = np.unique(data[:, column_2], return_counts=True)\n",
    "    result = 0\n",
    "    for i in range(len(values)):\n",
    "        newdata = data[data[:, column_2] == values[i]]\n",
    "        nb = count[i] / len(data)\n",
    "        result += nb * entropy(newdata, column_1)\n",
    "    return result\n",
    "\n",
    "def mutual_information(data: np.ndarray, column_1: int, column_2: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute mutual information I(X; Y)\n",
    "    \n",
    "    The gain in information is represented by the mutual information between 2 random variables\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data used to compute mutual information\n",
    "    column_1: int\n",
    "        column of X\n",
    "    column_2: int\n",
    "        column of Y\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return the result of I(X; Y) = H(X) - H(X|Y)\n",
    "    \"\"\"\n",
    "    return entropy(data, column_1) - conditionnal_entropy(data, column_1, column_2)\n",
    "\n",
    "# Choose the criteria randomly\n",
    "randnumber_1 = random.randint(0, 4)\n",
    "randnumber_2 = random.randint(0, 4)\n",
    "randnumber_3 = random.randint(0, 4)\n",
    "while randnumber_1 == randnumber_2:\n",
    "    randnumber_2 = random.randint(0, 4)\n",
    "while randnumber_3 == randnumber_1 or randnumber_3 == randnumber_2:\n",
    "    randnumber_3 = random.randint(0, 4)\n",
    "\n",
    "# Compute information gain for each criteria\n",
    "randnumbers = [randnumber_1, randnumber_2, randnumber_3]\n",
    "gain_information = []\n",
    "for i in range(3):\n",
    "    gain_information.append(mutual_information(data, 5, randnumbers[i]))\n",
    "    print(f\"For the criteria {header[randnumbers[i]]}, the gain of information is: {gain_information[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pour les mêmes critères de décision, calculer l’index Gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the criteria B, the gini index is: 0.5886499999999999\n",
      "For the criteria D, the gini index is: 0.7758499999999999\n",
      "For the criteria A, the gini index is: 0.5733999999999999\n"
     ]
    }
   ],
   "source": [
    "def gini(data: np.ndarray, column: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute the gini index\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data used to compute gini index\n",
    "    column: int\n",
    "        column used to compute gini index\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return the index gini\n",
    "    \"\"\"\n",
    "    values, count = np.unique(data[:, column], return_counts=True)\n",
    "    result = 0\n",
    "    for i in range(len(values)):\n",
    "        nb = count[i] / len(data)\n",
    "        result += nb * nb\n",
    "    return 1 - result\n",
    "\n",
    "gini_information = []\n",
    "for i in range(3):\n",
    "    gini_information.append(gini(data, randnumbers[i]))\n",
    "    print(f\"For the criteria {header[randnumbers[i]]}, the gini index is: {gini_information[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Quel est le critère de décision préférable selon le gain d’information ? Selon l’index Gini ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche à maximiser le gain d'information ou à minimiser l'index Gini.\n",
    "\n",
    "\n",
    "Le critère qui maximise le gain d'information n'est pas forcément égal au critère qui minimise l'index Gini.\n",
    "On a les résultats suivants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The criteria of decision which is preferable according to information gain is the criteria D\n",
      "The criteria of decision which is preferable according to gini index is the criteria A\n"
     ]
    }
   ],
   "source": [
    "index_of_max_gain = gain_information.index(max(gain_information))\n",
    "index_of_min_gini = gini_information.index(min(gini_information))\n",
    "print(f\"The criteria of decision which is preferable according to information gain is the criteria {header[randnumbers[index_of_max_gain]]}\")\n",
    "print(f\"The criteria of decision which is preferable according to gini index is the criteria {header[randnumbers[index_of_min_gini]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est pourquoi on verra dans la suite du travail pratique que un arbre construit avec l'index Gini est différent d'un arbre construit avec le gain d'information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2: ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implémenter l’algorithme ID3 avec comme critères possibles le gain d’information et l’index Gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The decision tree obtained thanks to information gain is different from decision tree obtained thanks to gini index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m tree \u001b[38;5;241m=\u001b[39m id3(data, header, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     76\u001b[0m tree_gini \u001b[38;5;241m=\u001b[39m id3(data, header, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tree \u001b[38;5;241m==\u001b[39m tree_gini, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe decision tree obtained thanks to information gain is different from decision tree obtained thanks to gini index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The decision tree obtained thanks to information gain is different from decision tree obtained thanks to gini index"
     ]
    }
   ],
   "source": [
    "def id3(data: np.ndarray, header: np.ndarray, index_data_to_train: int, use_gini: bool = False):\n",
    "    \"\"\"\n",
    "    This function compute a decision tree from a set of data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data used to compute the decision tree\n",
    "    header: np.ndarray\n",
    "        Header used to register nodes of the tree\n",
    "    index_data_to_train: int\n",
    "        Indicate which column of our data we want to train the decision tree for\n",
    "    use_gini: bool\n",
    "        Inidicate if we want to compute the tree thanks to index gini.\n",
    "        Default computation is made with information gain/mutual information\n",
    "        So the default value is set to False\n",
    "    \"\"\"\n",
    "    # Get the number of column of data\n",
    "    n = len(data[0])\n",
    "\n",
    "    # Case 1 column: count number of each element and return priority element\n",
    "    if n == 1:\n",
    "        values, count = np.unique(data[:], return_counts=True)\n",
    "        maximum = 0\n",
    "        index = 0\n",
    "        for i in range(len(values)):\n",
    "            if count[i] > maximum:\n",
    "                maximum = count[i]\n",
    "                index = i\n",
    "        return int(values[i])\n",
    "\n",
    "    # Test if there is some different values for the column to train\n",
    "    # If not, return the value\n",
    "    test_unique = np.unique(data[:, index_data_to_train])\n",
    "    if len(test_unique) == 1:\n",
    "        return int(test_unique[0])\n",
    "\n",
    "    # Compute mutual information between the column to train and each other column\n",
    "    # The column choosen maximise the mutual information\n",
    "    # In case we want to use gini index, we want to minimise the gini index.\n",
    "    maximum = 0\n",
    "    minimum = 1\n",
    "    index = 0\n",
    "    for i in range(n):\n",
    "        if i != index_data_to_train:\n",
    "            if use_gini:\n",
    "                info = gini(data, i)\n",
    "                if info < minimum:\n",
    "                    minimum = info\n",
    "                    index = i\n",
    "            else:\n",
    "                info = mutual_information(data, index_data_to_train, i)\n",
    "                if info > maximum:\n",
    "                    maximum = info\n",
    "                    index = i\n",
    "\n",
    "    # Get all the values of the column choosen.\n",
    "    # For each values, compute id3 on a new data with just the line\n",
    "    # where the value of the column choosen is equal to the selected value\n",
    "    # and with the column choosen deleted.\n",
    "    # Compute a tree thanks to the results of id3 for each new data\n",
    "    values = np.unique(data[:, index])\n",
    "    tree = {}\n",
    "    for i in values:\n",
    "        newdata = np.delete(data[data[:, index] == i], index, 1)\n",
    "        newheader = np.delete(header, index, 0)\n",
    "        if (index < index_data_to_train):\n",
    "            tree[int(i)] = id3(newdata, newheader, index_data_to_train - 1, use_gini)\n",
    "        else:\n",
    "            tree[int(i)] = id3(newdata, newheader, index_data_to_train, use_gini)\n",
    "    \n",
    "    # Return the tree obtained\n",
    "    return {header[index]: tree}\n",
    "\n",
    "tree = id3(data, header, 5)\n",
    "tree_gini = id3(data, header, 5, True)\n",
    "\n",
    "assert tree == tree_gini, \"The decision tree obtained thanks to information gain is different from decision tree obtained thanks to gini index\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Comparer l’arbre obtenu à l’aide d’ID3 gain d’information avec celui produit par la démonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de savoir si les arbres obtenus sont identiques, je vais mesurer différentes informations comme la profondeur maximale de l'arbre, et choisir différents chemins pour voir si par hasard les chemins obtenus avec mon arbre sont identiques aux chemins obtenus avec l'arbre obtenu de la démonstration.\n",
    "\n",
    "Tout d'abord, on a la profondeur maximale de l'arbre de la démonstration qui est donné par 4 critères pour arriver à une décision.\n",
    "Pour mesurer la profondeur de mon arbre, j'ai implémenté les fonctions suivantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My decision tree\n",
      "{'E': {0: {'C': {0: 0, 1: 1, 2: {'A': {0: 1, 1: {'B': {0: 0, 1: 1}}}}, 3: 1}}, 1: {'D': {0: {'A': {1: 0, 2: 1}}, 1: {'B': {0: {'A': {0: 0, 2: 1}}, 1: 0, 2: {'C': {0: 1, 1: 0}}}}, 2: {'A': {0: 0, 2: {'B': {0: 1, 1: 0}}}}, 3: 0, 4: 0}}, 2: {'D': {0: 1, 1: {'B': {0: 1, 1: 0, 2: {'C': {0: 1, 1: 0}}}}, 2: 0, 3: 0, 4: 0}}}}\n",
      "Depth of my tree: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"My decision tree\")\n",
    "print(tree)\n",
    "\n",
    "def compute_max_path(tree: dict) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    This function go throught the decision tree and return the maximum path from root node to leaf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        A decision tree\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    The maximum path of the tree\n",
    "    \"\"\"\n",
    "    if type(tree) != int and len(list(tree.keys())) != 0:\n",
    "        item = list(tree.keys())[0]\n",
    "        tree = tree[item]\n",
    "        if type(tree) != int:\n",
    "            keys = list(tree.keys())\n",
    "            maximum = 0\n",
    "            for i in range(len(keys)):\n",
    "                mypath = compute_max_path(tree) + 1\n",
    "                if mypath > maximum:\n",
    "                    maximum = mypath\n",
    "        else:\n",
    "            return 1\n",
    "    return maximum\n",
    "\n",
    "print(f\"Depth of my tree: {compute_max_path(tree)}\")\n",
    "\n",
    "assert compute_max_path(tree) == 4, \"Depth of trees must be equal so that the trees are similar\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut constater que notre arbre a la même profondeur que l'arbre donné dans la démonstration.\n",
    "Donc on est en bonne voie pour avoir deux arbres identiques.\n",
    "\n",
    "Maintenant, on va prendre une dizaine de données et voir si les chemins dans les arbres sont les mêmes pour ces 10 données.\n",
    "On choisit les données suivantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 1 1 0]\n",
      "[0 1 2 1 2 0]\n",
      "[0 0 3 4 0 1]\n",
      "[0 1 1 3 1 0]\n",
      "[0 0 1 3 0 1]\n",
      "[0 0 2 3 1 0]\n",
      "[1 1 1 3 0 1]\n",
      "[0 0 2 4 1 0]\n",
      "[2 1 2 2 1 0]\n",
      "[1 2 1 3 0 1]\n",
      "[2 1 2 1 2 0]\n",
      "[0 1 1 1 2 0]\n",
      "[0 1 3 3 1 0]\n",
      "[2 2 3 0 1 1]\n",
      "[0 1 1 4 2 0]\n",
      "[2 2 3 0 1 1]\n",
      "[0 1 2 2 2 0]\n",
      "[0 1 2 4 1 0]\n",
      "[0 0 1 4 1 0]\n",
      "[2 2 2 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les chemins obtenus pour chacune des données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 1 1 0] -> ['E', 'D', 'B']\n",
      "[0 1 2 1 2 0] -> ['E', 'D', 'B']\n",
      "[0 0 3 4 0 1] -> ['E', 'C']\n",
      "[0 1 1 3 1 0] -> ['E', 'D']\n",
      "[0 0 1 3 0 1] -> ['E', 'C']\n",
      "[0 0 2 3 1 0] -> ['E', 'D']\n",
      "[1 1 1 3 0 1] -> ['E', 'C']\n",
      "[0 0 2 4 1 0] -> ['E', 'D']\n",
      "[2 1 2 2 1 0] -> ['E', 'D', 'A', 'B']\n",
      "[1 2 1 3 0 1] -> ['E', 'C']\n",
      "[2 1 2 1 2 0] -> ['E', 'D', 'B']\n",
      "[0 1 1 1 2 0] -> ['E', 'D', 'B']\n",
      "[0 1 3 3 1 0] -> ['E', 'D']\n",
      "[2 2 3 0 1 1] -> ['E', 'D', 'A']\n",
      "[0 1 1 4 2 0] -> ['E', 'D']\n",
      "[2 2 3 0 1 1] -> ['E', 'D', 'A']\n",
      "[0 1 2 2 2 0] -> ['E', 'D']\n",
      "[0 1 2 4 1 0] -> ['E', 'D']\n",
      "[0 0 1 4 1 0] -> ['E', 'D']\n",
      "[2 2 2 0 1 1] -> ['E', 'D', 'A']\n"
     ]
    }
   ],
   "source": [
    "def get_path(tree: dict, data: list, header: list) -> list:\n",
    "    \"\"\"\n",
    "    This function go throught the decision tree and return a random path from root node to leaf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        A decision tree\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    A dictionnary with the random path created\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    header = list(header)\n",
    "    while type(tree) != int and len(list(tree.keys())) != 0:\n",
    "        item = list(tree.keys())[0]\n",
    "        result.append(item)\n",
    "        tree = tree[item]\n",
    "        if type(tree) != int:\n",
    "            tree = tree[data[header.index(item)]]\n",
    "    return result\n",
    "mytree_paths = []\n",
    "for i in range(20):\n",
    "    mytree_paths.append(get_path(tree, data[i], header))\n",
    "    print(f\"{data[i]} -> {mytree_paths[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les chemins obtenus pour chacune des données dans l'arbre de décision de démonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_paths = [\n",
    "    ['E', 'D', 'B'],\n",
    "    ['E', 'D', 'B'],\n",
    "    ['E', 'C'],\n",
    "    ['E', 'D'],\n",
    "    ['E', 'C'],\n",
    "    ['E', 'D'],\n",
    "    ['E', 'C'],\n",
    "    ['E', 'D'],\n",
    "    ['E', 'D', 'A', 'B'],\n",
    "    ['E', 'C'],\n",
    "    ['E', 'D', 'B'],\n",
    "    ['E', 'D', 'B'],\n",
    "    ['E', 'D'],\n",
    "    ['E', 'D', 'A'],\n",
    "    ['E', 'D'],\n",
    "    ['E', 'D', 'A'],\n",
    "    ['E', 'D'],\n",
    "    ['E', 'D'],\n",
    "    ['E', 'D'],\n",
    "    ['E', 'D', 'A']\n",
    "]\n",
    "\n",
    "assert demo_paths == mytree_paths, \"Error: paths are different for given values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut donc remarquer que les chemins sont les mêmes pour les mêmes données de nos deux arbres.\n",
    "\n",
    "De plus, si on compare les deux arbres, on a 'E' qui est le noeud racine dans les 2 arbres.\n",
    "Si on a une valeur de 0, on entre dans le sous arbre de noeud racine 'C' pour les 2 arbres.\n",
    "Puis, si on a une valeur de 0, dans les 2 arbres on obtient une décision de 0,\n",
    "et si on a une valeur de 1, on obtient une décision de 1, et si on a une valeur de 2, on obtient une décision d'un sous-arbre de sommet 'A' dans les deux arbres, et si on a une valeur de 3, on obtient une valeur de 1 dans les 2 arbres.\n",
    "\n",
    "Dans les deux arbres, le sous-arbre 'A' possède la valeur 0 qui ammène au choix 1, et la valeur 1 qui ammène au sous-arbre de sommet 'B'.\n",
    "\n",
    "Le sous arbre de sommet 'B' possède 2 valeurs: une valeur 1 menant à un choix de 1, et une valeur 0 menant à un choix de 0 dans les deux arbres.\n",
    "\n",
    "Donc la partie vérifiée de l'arbre de la démonstration est identique à la partie véfiriée de mon arbre.\n",
    "\n",
    "Donc on a les deux arbres qui ont la même profondeur, qui ont les mêmes chemins emprunté pour les mêmes données, et qui possèdent une structure qui est en partie la même (je n'ai pas fait explicitement l'analyse de la structure des arbres pour le reste des arbres, mais c'est la même chose.)\n",
    "\n",
    "Tout ces éléments nous laissent supposer avec très forte raison que les deux arbres sont identiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implémenter une procédure de génération de données à partir d’un arbre de décision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 2 1]\n",
      " [1 1 4 0 2 1]\n",
      " [0 0 0 1 0 0]\n",
      " [0 2 2 0 0 1]\n",
      " [0 2 1 1 0 1]\n",
      " [1 1 1 1 0 1]\n",
      " [3 1 3 3 2 0]\n",
      " [0 1 2 3 0 1]\n",
      " [0 2 1 3 0 1]\n",
      " [0 2 1 1 1 0]\n",
      " [0 2 0 2 2 0]\n",
      " [0 0 0 2 1 0]\n",
      " [1 1 2 0 0 1]\n",
      " [0 2 2 1 0 1]\n",
      " [1 0 2 3 0 0]\n",
      " [0 1 1 4 2 0]\n",
      " [0 1 0 3 0 0]\n",
      " [1 1 0 4 1 0]\n",
      " [0 2 3 0 0 1]\n",
      " [0 1 1 2 2 0]\n",
      " [3 2 3 2 2 0]\n",
      " [1 1 3 1 0 1]\n",
      " [3 2 4 0 2 1]\n",
      " [1 0 4 3 1 0]\n",
      " [0 1 1 1 1 0]\n",
      " [1 2 0 3 2 0]\n",
      " [0 0 4 2 1 0]\n",
      " [0 2 1 4 1 0]\n",
      " [0 2 2 1 0 1]\n",
      " [0 2 2 1 0 1]\n",
      " [0 1 4 2 1 0]\n",
      " [2 0 4 2 1 1]\n",
      " [3 0 4 3 1 0]\n",
      " [3 0 3 1 2 1]\n",
      " [2 0 3 0 1 1]\n",
      " [1 2 3 3 2 0]\n",
      " [1 2 0 0 0 0]\n",
      " [0 2 0 4 2 0]\n",
      " [1 1 0 1 0 0]\n",
      " [3 1 1 1 1 0]\n",
      " [0 0 0 3 2 0]\n",
      " [0 2 4 2 1 0]\n",
      " [1 1 3 4 1 0]\n",
      " [0 2 1 0 2 1]\n",
      " [1 0 3 0 0 1]\n",
      " [0 0 0 1 1 0]\n",
      " [0 2 4 3 2 0]\n",
      " [3 0 0 1 2 1]\n",
      " [0 1 3 1 0 1]\n",
      " [0 0 0 3 1 0]\n",
      " [1 0 4 2 2 0]\n",
      " [1 2 4 0 1 0]\n",
      " [3 0 0 0 2 1]\n",
      " [0 2 2 0 0 1]\n",
      " [1 1 2 0 0 1]\n",
      " [1 0 3 1 0 1]\n",
      " [2 2 4 0 1 1]\n",
      " [1 0 2 1 0 0]\n",
      " [3 0 1 0 0 1]\n",
      " [1 1 0 0 0 0]\n",
      " [1 0 0 2 2 0]\n",
      " [0 1 3 2 1 0]\n",
      " [1 0 4 1 2 1]\n",
      " [1 2 0 0 0 0]\n",
      " [3 0 0 4 1 0]\n",
      " [1 1 0 3 2 0]\n",
      " [0 2 2 0 0 1]\n",
      " [2 0 0 2 1 1]\n",
      " [0 1 3 4 1 0]\n",
      " [1 0 0 3 1 0]\n",
      " [0 2 4 2 2 0]\n",
      " [3 2 3 2 2 0]\n",
      " [1 0 4 3 2 0]\n",
      " [3 1 3 0 0 1]\n",
      " [0 0 2 3 0 1]\n",
      " [1 2 0 0 1 0]\n",
      " [0 0 1 1 1 0]\n",
      " [3 1 1 1 1 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 2 3 0 0 1]\n",
      " [1 0 3 0 1 0]\n",
      " [1 0 1 3 0 1]\n",
      " [3 2 4 3 2 0]\n",
      " [1 2 4 4 2 0]\n",
      " [1 2 3 1 0 1]\n",
      " [3 1 3 4 2 0]\n",
      " [1 0 1 4 1 0]\n",
      " [3 0 0 0 0 0]\n",
      " [1 2 0 2 2 0]\n",
      " [0 1 1 0 2 1]\n",
      " [1 1 4 1 1 0]\n",
      " [0 1 1 0 0 1]\n",
      " [1 2 3 0 1 0]\n",
      " [3 1 3 1 0 1]\n",
      " [3 2 1 4 1 0]\n",
      " [0 2 0 0 2 1]\n",
      " [3 1 0 1 2 0]\n",
      " [2 0 1 2 1 1]\n",
      " [2 0 0 1 1 1]\n",
      " [3 2 4 4 2 0]]\n"
     ]
    }
   ],
   "source": [
    "def gen_data(tree: dict, item_unknow: str) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    This function go throught the decision tree and return a random path from root node to leaf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        A decision tree\n",
    "    item_unknow: str\n",
    "        Name of the unknow item\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    A dictionnary with the random path created\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    mytree = tree\n",
    "    while type(mytree) != int and len(list(mytree.keys())) != 0:\n",
    "        item = list(mytree.keys())[0]\n",
    "        mytree = mytree[item]\n",
    "        if type(mytree) != int:\n",
    "            keys = list(mytree.keys())\n",
    "            randnumber = random.randint(0, len(keys) - 1)\n",
    "            data[item] = keys[randnumber]\n",
    "            mytree = mytree[keys[randnumber]]\n",
    "        else:\n",
    "            data[item_unknow] = mytree\n",
    "    if type(mytree) == int:\n",
    "        data[item_unknow] = mytree\n",
    "\n",
    "    return data\n",
    "\n",
    "def complete_gen_data(tree: dict, item_unknow: str, header: list, definition_domain: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Allow to complete datas generated by a path in the tree by adding missing fields\n",
    "    and random values taken from the definition domain of this fields for each missing field\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        A decision tree\n",
    "    item_unknow: str\n",
    "        Name of the unknow item\n",
    "    header: np.array\n",
    "        Name of each criteria\n",
    "    definition_domain: list\n",
    "        Definition domain of each criteria\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return a np.array with the value assignated for each criteria in the order given by the header\n",
    "    \"\"\"\n",
    "    data = gen_data(tree, item_unknow)\n",
    "    for i in range(len(header)):\n",
    "        item = header[i]\n",
    "        if item not in data:\n",
    "            values = definition_domain[i]\n",
    "            random_number = random.randint(0, len(definition_domain[i]) - 1)\n",
    "            data[item] = values[random_number]\n",
    "    newdata = []\n",
    "    for i in range(len(header)):\n",
    "        newdata.append(data[header[i]])\n",
    "    return np.array(newdata)\n",
    "\n",
    "def gen_multiple_datas(tree: dict, item_unknow: str, header: list, definition_domain: list, number: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Allow to generate multiple datas from a tree\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        A decision tree\n",
    "    item_unknow:\n",
    "        Name of the unknow item\n",
    "    header: list\n",
    "        Name of the criteria\n",
    "    definition_domain: list\n",
    "        Definition domain of each criteria\n",
    "    number: int\n",
    "        Number of data which would be generated\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return a np.array of all the datas generated thanks to the decision tree\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(number):\n",
    "        result.append(complete_gen_data(tree, item_unknow, header, definition_domain))\n",
    "    return np.array(result)\n",
    "\n",
    "# Create the definition domain of each criteria\n",
    "definition_domain = []\n",
    "for i in range(len(header)):\n",
    "    definition_domain.append(np.unique(data[i]))\n",
    "\n",
    "# Example of execution of the function to generate datas\n",
    "print(gen_multiple_datas(tree, 'c', header, definition_domain, 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. À l’aide d’ID3 gain d’information, construire 5 arbres à partir d’échantillons aléatoires de 80% des données et utiliser comme prédiction finale un vote de majorité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_of_data(data: np.ndarray, percentage: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Allow to get random sampling of a percentage of datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Datas we want a percentage of\n",
    "    percentage: int\n",
    "        Percentage of datas we want\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return a tuple with the np.array with the right percentage of the datas we want,\n",
    "    and with the np.array of the datas which weren't taken\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(data))\n",
    "    indices = np.unique((indices * percentage/100).round().astype(int))\n",
    "    last_indices = np.arange(len(data))\n",
    "    mask = np.isin(last_indices, indices).astype(int)\n",
    "    mask -= 1\n",
    "    mask[mask < 0] = 1\n",
    "    last_indices = np.unique(last_indices * mask).astype(int)\n",
    "    newdata = data.copy()\n",
    "    np.random.shuffle(newdata)\n",
    "    return newdata[indices], newdata[last_indices[1:]]\n",
    "\n",
    "# Construct the five decision trees here\n",
    "trees = []\n",
    "training = []\n",
    "tests = []\n",
    "for i in range(5):\n",
    "    data_training, data_tests = get_percentage_of_data(data, 80)\n",
    "    training.append(data_training)\n",
    "    tests.append(data_tests)\n",
    "    trees.append(id3(data_training, header, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Comparer les performances du premier arbre obtenu avec celles de l’ensemble de 5 arbres selon les métriques suivantes: accuracy, precision, recall, F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree\n",
      "accuracy: 0.22\n",
      "precision (p, r): (0.6504065040650407, 0.9876543209876543)\n",
      "F1 score: 0.7843137254901962\n",
      "\n",
      "Average of 5 trees\n",
      "accuracy: 0.265\n",
      "precision (p, r): (0.6520325203252033, 0.8927274852485038)\n",
      "F1 score: 0.7515041947305535\n"
     ]
    }
   ],
   "source": [
    "# Define the labels here used to evaluation\n",
    "FN, FP, TN, TP = range(4)\n",
    "\n",
    "def eval(tree: dict, data: np.ndarray, header: np.ndarray, target: int) -> int:\n",
    "    \"\"\"\n",
    "    Allow to eval a specific data, with a target value, thanks to a given decision tree\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        Decision tree\n",
    "    data: np.ndarray\n",
    "        Data we want to evaluate\n",
    "    header: np.ndarray\n",
    "        Header of data\n",
    "    target: int\n",
    "        target column of decision value of data\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return a label in {FN, FP, TN, TP}\n",
    "    \"\"\"\n",
    "    data = np.array(data).astype(int)\n",
    "    while type(tree) != int:\n",
    "        keys = list(tree.keys())\n",
    "        index = np.where(header == keys[0])[0][0]\n",
    "        tree = tree[header[index]]\n",
    "        keys = list(tree.keys())\n",
    "        if data[index] not in keys:\n",
    "            if data[target] == 0:\n",
    "                return TN\n",
    "            return FP\n",
    "        tree = tree[data[index]]\n",
    "    if data[target] == 1 and tree == 1:\n",
    "        return TP\n",
    "    if data[target] == 0 and tree == 1:\n",
    "        return FN\n",
    "    if data[target] == 0 and tree == 0:\n",
    "        return TN\n",
    "    if data[target] == 1 and tree == 0:\n",
    "        return FP\n",
    "\n",
    "def evaluation(tree: dict, datas: np.ndarray, header: np.ndarray, target: int):\n",
    "    \"\"\"\n",
    "    Evaluate a set of datas for a specific decision tree\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        Decision tree\n",
    "    datas: np.ndarray\n",
    "        Datas we want to evaluate\n",
    "    header: np.ndarray\n",
    "        Header of datas\n",
    "    target: int\n",
    "        target column of decision value of datas\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return a list of the ampiric probability to have respectively FN, FP, TN and TP.\n",
    "    \"\"\"\n",
    "    n = len(datas)\n",
    "    FN_count = 0\n",
    "    FP_count = 0\n",
    "    TN_count = 0\n",
    "    TP_count = 0\n",
    "    for i in datas:\n",
    "        result = eval(tree, i, header, target)\n",
    "        if result == FN: FN_count += 1\n",
    "        if result == FP: FP_count += 1\n",
    "        if result == TN: TN_count += 1\n",
    "        if result == TP: TP_count += 1\n",
    "    return FN_count/n, FP_count/n, TN_count/n, TP_count/n\n",
    "\n",
    "def accuracy(tree: dict, test_datas: np.ndarray, header: np.ndarray, target: int) -> float:\n",
    "    \"\"\"\n",
    "    Return the error rate of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return the probability of making an error in our prediction\n",
    "    \"\"\"\n",
    "    fn, fp, tn, tp = evaluation(tree, test_datas, header, target)\n",
    "    return (fp + fn)/(tp + tn + fp + fn)\n",
    "\n",
    "def precision(tree, test_datas: np.ndarray, header: np.ndarray, target: int):\n",
    "    \"\"\"\n",
    "    Return the precision of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "\n",
    "    Return value TODO !!!\n",
    "    ------------\n",
    "    Return the precision and the recall of the tree\n",
    "    \"\"\"\n",
    "    fn, fp, tn, tp = evaluation(tree, test_datas, header, target)\n",
    "    return tp / (tp + fp), tp / (tp + fn)\n",
    "\n",
    "def f1_score(tree, test_datas: np.ndarray, header: np.ndarray, target: int) -> float:\n",
    "    \"\"\"\n",
    "    Return the f1 score of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    Return the f1 score of the tree\n",
    "    \"\"\"\n",
    "    p, r = precision(tree, test_datas, header, target)\n",
    "    return (2 * p * r) / (p + r)\n",
    "\n",
    "\n",
    "# Get data test\n",
    "_, data_test = convert_csv2array('data_test.csv')\n",
    "\n",
    "# Get metrics of the tree computed with 100% of the datas\n",
    "accuracy_target = accuracy(tree, data_test, header, 5)\n",
    "precision_target_p, precision_target_r = precision(tree, data_test, header, 5)\n",
    "f1_score_target = f1_score(tree, data_test, header, 5)\n",
    "\n",
    "# Get average metrics of the 5 threes computed with 80% of the datas\n",
    "average_accuracy = 0\n",
    "average_precision_p = 0\n",
    "average_precision_r = 0\n",
    "average_f1_score = 0\n",
    "for i in range(len(trees)):\n",
    "    average_accuracy += accuracy(trees[i], data_test, header, 5)\n",
    "    p, r = precision(trees[i], data_test, header, 5)\n",
    "    average_precision_p += p\n",
    "    average_precision_r += r\n",
    "    average_f1_score += f1_score(trees[i], data_test, header, 5)\n",
    "\n",
    "average_accuracy /= len(trees)\n",
    "average_precision_p /= len(trees)\n",
    "average_precision_r /= len(trees)\n",
    "average_f1_score /= len(trees)\n",
    "\n",
    "# Print results\n",
    "print(\"Tree\")\n",
    "print(\"accuracy: \" + str(accuracy_target))\n",
    "print(\"precision (p, r): (\" + str(precision_target_p) + \", \" + str(precision_target_r) + \")\")\n",
    "print(f\"F1 score: {f1_score_target}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Average of 5 trees\")\n",
    "print(f\"accuracy: {average_accuracy}\")\n",
    "print(f\"precision (p, r): ({average_precision_p}, {average_precision_r})\")\n",
    "print(f\"F1 score: {average_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut donc observer que le taux d'erreur (acurracy) de l'arbre entraîné avec 100% des données est plus faible que la moyenne du taux d'erreur des 5 arbres entraînés avec 80% des données.\n",
    "\n",
    "Cependant, on peut observer que la précision p de l'arbre est plus petite que la précision p de la moyenne des arbres, ce qui signifie que\n",
    "$$p_1 = \\frac{tp_1}{tp_1 + fp_1} \\lt p_2 = \\frac{tp_2}{tp_2 + fp_2}$$\n",
    "\n",
    "Mais on peut observer que l'on a également le recall de l'arbre qui est plus grand que le recall de la moyenne des arbres.\n",
    "$$r_1 = \\frac{tp_1}{tp_1 + fn_1} \\gt r_2 = \\frac{tp_2}{tp_2 + fn_2}$$\n",
    "\n",
    "Le $F_1$ score permet alors de comparer les deux métriques obtenues:\n",
    "\n",
    "$$F_1 = \\frac{2}{(\\frac{1}{p} + \\frac{1}{r})}$$\n",
    "$$= \\frac{2}{\\frac{r+p}{rp}}$$\n",
    "$$= \\frac{2rp}{r + p}$$\n",
    "\n",
    "Dans notre cas, le $F_1$ score de l'arbre entraîné avec toutes les données est plus grand que le $F_1$ score moyen des autres arbres obtenus avec 80% des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Selon le F1 score, quel modèle devrait être privilégié ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre $F_1$ score peut s'écrire sous la forme:\n",
    "\n",
    "$$F_1 = \\frac{2}{(\\frac{1}{p} + \\frac{1}{r})}$$\n",
    "$$= \\frac{2}{\\frac{tp + fn}{tp} + \\frac{tp + fp}{tp}}$$\n",
    "$$= \\frac{tp}{\\frac{1}{2}(tp + fn + tp + fp)}$$\n",
    "$$= \\frac{tp}{tp + \\frac{1}{2}(fp + fn)}$$\n",
    "\n",
    "Comme notre but est de minimiser les `false positive (fp)` et les `false negative (fn)`, le meilleur $F_1$ score est le $F_1$ score le plus grand, car plus `(fp + fn)` est grand, plus le $F_1$ score est petit.\n",
    "\n",
    "Dans notre cas, le $F_1$ score obtenu est plus grand pour l'arbre entraîné avec 100% des données, donc je pense que ce modèle devrait être privilégié."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
