{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Travaux pratiques d’IA\n",
    "## <center> Série 5: Naive Bayes & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données\n",
    "1. Les données à utiliser pour ce TP se trouvent dans les fichiers data_train.csv et data_test csv.\n",
    "    Il s’agit de prédire l’achat d’un produit en fonction du sexe, de l’âge et du salaire d’un individu.\n",
    "2. Les 3 premières colonnes spécifient les covariables tandis que la dernière colonne correspond aux labels.\n",
    "3. Le fichier data_test.csv sert à évaluer les performances des modèles développés à partir des données du fichier data_train.csv.\n",
    "4. Il est recommandé d’utiliser pandas et/ou NumPy pour manipuler les données. Notamment la méthode get_dummies de pandas vous permet de convertir la première covariable en deux covariables binaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Naive Bayes\n",
    "Le but de cette section est d’implémenter Naive Bayes. Voici les différentes étapes à accomplir:\n",
    "\n",
    "1. Calculer la distribution empirique des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def convert_csv2array(name: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert a csv file into numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: string\n",
    "        Path to the file\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    header and data of csv\n",
    "    \"\"\"\n",
    "    file = open(name, 'r')\n",
    "    data = []\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "    data = np.array(data)\n",
    "    return data[0, :], data[1:, :]\n",
    "\n",
    "header, data = convert_csv2array('data_train.csv')\n",
    "\n",
    "def ampiric_distribution(data: np.ndarray, target: int) -> float:\n",
    "    \"\"\"\n",
    "    Get parameter of bernoulli distribution from a set of values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        set of values\n",
    "    target: int\n",
    "        Column target to compute ampiric distribution\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    The parameter p of the bernoulli distribution\n",
    "    \"\"\"\n",
    "    return np.sum(data[:, target].astype(int))/len(data)\n",
    "\n",
    "print(ampiric_distribution(data, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pour chaque valeur des labels, estimer les paramètres des distributions des covariables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': (0.7, {'Gender': {'Female': 0.49159663865546216, 'Male': 0.5084033613445378}, 'Age': {'Mean': 32.26890756302521, 'Variance': 64.32264670574112}, 'EstimatedSalary': {'Mean': 60100.84033613445, 'Variance': 625023444.6719865}}), '1': (0.3, {'Gender': {'Female': 0.5490196078431373, 'Male': 0.45098039215686275}, 'Age': {'Mean': 45.0, 'Variance': 79.66666666666667}, 'EstimatedSalary': {'Mean': 96549.01960784313, 'Variance': 1617855440.2153018}})}\n"
     ]
    }
   ],
   "source": [
    "def distrib_param(data: np.ndarray, header: np.ndarray, target: int):\n",
    "    \"\"\"\n",
    "    Get parameters of all distributions of a set of label and covariables\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data to compute\n",
    "    header: np.ndarray\n",
    "        Header of the data\n",
    "    target: int\n",
    "        Column target for the label\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    dict contain labels with parameter of the distribution of the labels, and parameter of the distributions of each variable\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    values = np.unique(data[:, target])\n",
    "    for i in range(len(values)):\n",
    "        newdata = data[data[:, target] == values[i]]\n",
    "        tmp = {}\n",
    "        for j in range(len(data[0])):\n",
    "            if j != target:\n",
    "                tmp_2 = {}\n",
    "                newvalues, count = np.unique(newdata[:, j], return_counts = True)\n",
    "                if len(newvalues) == 2:\n",
    "                    for k in range(len(newvalues)):\n",
    "                        tmp_2[newvalues[k]] = count[k] / len(newdata)\n",
    "                else:\n",
    "                    tmp_2[\"Mean\"] = np.mean(newdata[:, j].astype(float))\n",
    "                    tmp_2[\"Variance\"] = np.var(newdata[:, j].astype(float))\n",
    "                tmp[header[j]] = tmp_2\n",
    "        result[values[i]] = (len(newdata) / len(data), tmp)\n",
    "    return result\n",
    "\n",
    "print(distrib_param(data, header, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implémenter la fonction de densité gausienne et la fonction de probabilité de Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_density_function(x: float, mean: float, var: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the probability of x for a given gaussian density function, obtained thanks to mean and variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: float\n",
    "        value given to the pdf\n",
    "    mean: float\n",
    "        mean of the gaussian distribution\n",
    "    var: float\n",
    "        variance of the gaussian distribution\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    The probability of x\n",
    "    \"\"\"\n",
    "    return (1/np.sqrt(var * 2 * np.pi)) * np.exp((-1/2)* ((x - mean)/np.sqrt(var))**2)\n",
    "\n",
    "def bernoulli_density_function(x: float, p: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the probability of x for a given bernoulli density function, obtained thanks to p parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: float\n",
    "        value given to the pdf\n",
    "    p: float\n",
    "        parameter of the bernoulli distribution\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    The probability of x\n",
    "    \"\"\"\n",
    "    return (p ** x)((1 - p) ** (1 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Étant donné de nouvelles covariables, prédire les labels correspondants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2833333333333333, 0.0, 0.31666666666666665, 0.4)\n"
     ]
    }
   ],
   "source": [
    "# The `naive_base` function is a classification algorithm that uses a naive Bayes classifier. It takes three parameters: `distrib`, `header`, and `data`.\n",
    "def naive_base(distrib: dict, header: np.ndarray, data: np.ndarray):\n",
    "    \"\"\"\n",
    "    This function is a naive Bayes classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distrib: dict\n",
    "        Parameter of all distributions obtained thanks to a set of data\n",
    "    header: np.ndarray\n",
    "        header of the data\n",
    "    data: np.ndarray\n",
    "        data to estimate\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    The estimation of the given data    \n",
    "    \"\"\"\n",
    "    values_keys = list(distrib.keys())\n",
    "    maximum = 0\n",
    "    value = values_keys[0]\n",
    "    for i in values_keys:\n",
    "        denom = 1\n",
    "        for j in range(len(header)):\n",
    "            if list(distrib[i][1][header[j]].keys())[0] == \"Mean\" or list(distrib[i][1][header[j]].keys())[1] == \"Mean\":\n",
    "                denom *= gaussian_density_function(float(data[j]), distrib[i][1][header[j]][\"Mean\"], distrib[i][1][header[j]][\"Variance\"])\n",
    "            else:\n",
    "                denom *= distrib[i][1][header[j]][data[j]]\n",
    "\n",
    "        if denom * distrib[i][0] > maximum:\n",
    "            maximum = denom\n",
    "            value = i\n",
    "    return int(value)\n",
    "\n",
    "FN, FP, TN, TP = range(4)\n",
    "\n",
    "# The `eval_naive_base` function is used to evaluate the performance of the `naive_base` function on a given test dataset.\n",
    "def eval_naive_base(data: np.ndarray, header: np.ndarray, data_test: np.ndarray, target: int):\n",
    "    \"\"\"\n",
    "    Function used to evaluate the performance of the naive base classifier on a given test dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data used to train the classifier\n",
    "    header: np.ndarray\n",
    "        header of the data\n",
    "    data_test: np.ndarray\n",
    "        test dataset\n",
    "    target: int\n",
    "        Target column contains label\n",
    "\n",
    "    Return values\n",
    "    -------------\n",
    "    The probability of false negative, false positive, true negative, true positive\n",
    "    \"\"\"\n",
    "    target_values = data_test[:, target].astype(int)\n",
    "    data_test = np.delete(data_test, target, 1)\n",
    "    header_test = np.delete(header, target)\n",
    "\n",
    "    FN_count = 0\n",
    "    FP_count = 0\n",
    "    TN_count = 0\n",
    "    TP_count = 0\n",
    "    for i in range(len(data_test)):\n",
    "        result = naive_base(distrib_param(data, header, target), header_test, data_test[i])\n",
    "\n",
    "        if result == 1 and target_values[i] == 1:\n",
    "            TP_count += 1\n",
    "        elif result == 0 and target_values[i] == 0:\n",
    "            TN_count += 1\n",
    "        elif result == 1 and target_values[i] == 0:\n",
    "            FP_count += 1\n",
    "        elif result == 0 and target_values[i] == 1:\n",
    "            FN_count += 1\n",
    "    return FN_count / len(data_test), FP_count / len(data_test), TN_count / len(data_test), TP_count / len(data_test)\n",
    "\n",
    "\n",
    "_, data_test = convert_csv2array('data_test.csv')\n",
    "\n",
    "print(eval_naive_base(data, header, data_test, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Logistic Regression\n",
    "Le but de cette section est d’implémenter Logistic Regression. On suppose donc le modèle\n",
    "suivant:\n",
    "$$yi \\approx^{ind} Bernoulli(p_i), p_i = \\sigma(w^T x_i + b), \\sigma(z) = \\frac{1}{1+exp−z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sur papier, dériver:\n",
    "\n",
    "(a) $p(y_i|x_i; w, b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y_i|x_i; w, b) = \\boxed{p_i^{y_i}(1 - p_i)^{1 - y_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) $log(p(y_i|x_i; w, b))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log(p(y_i|x_i; w, b))$$\n",
    "$$= log(p_i^{y_i}(1 - p_i)^{1 - y_i})$$\n",
    "$$= log(p_i^{y_i}) + log((1 - p_i)^{1 - y_i})$$\n",
    "$$= \\boxed{y_i log(p_i) + (1 - y_i)log(1 - p_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) $\\frac{d\\sigma(z)}{dz}$ comme une fonction de $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d\\sigma(z)}{dz}$$\n",
    "$$= ((1 + e^{-z})^{-1})'$$\n",
    "$$= -1 \\times - e ^{-z} \\times (1 + e^{-z})^{-2}$$\n",
    "$$=\\frac{e^{-z}}{(1 + e^{-z})^2}$$\n",
    "$$=\\frac{1}{1 + e^{-z}}\\frac{e^{-z}}{1 + e^{-z}}$$\n",
    "$$= \\sigma (z) \\frac{e^{-z}}{1 + e^{-z}}$$\n",
    "$$= \\sigma (z) \\frac{1 + e^{-z} - 1}{1 + e^{-z}}$$\n",
    "$$= \\sigma (z) (\\frac{1 + e^{-z}}{1 + e^{-z}} - \\frac{1}{1 + e^{-z}})$$\n",
    "$$= \\sigma (z) (1 - \\frac{1}{1 + e^{-z}})$$\n",
    "$$= \\boxed{\\sigma (z) (1 - \\sigma (z))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) $\\frac{dlog(p(y_i|x_i;w,b))}{dw_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dlog(p(y_i|x_i;w,b))}{dw_j}$$\n",
    "\n",
    "$$=\\frac{y_i log(p_i) + (1 - y_i)log(1 - p_i)}{dw_j}$$\n",
    "$$=y_i \\frac{log(p_i)}{dw_j} + (1 - y_i)\\frac{log(1 - p_i)}{dw_j}$$\n",
    "$$=y_i \\frac{log(\\sigma (z))}{dw_j} + (1 - y_i)\\frac{log(1 - \\sigma (z))}{dw_j},\\ z = w^T x_i + b$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dw_j}\\sigma (z) + (1 - y_i)\\frac{1}{1 - \\sigma (z)}\\frac{d}{dw_j}(1 - \\sigma (z))$$\n",
    "\n",
    "On a:\n",
    "$$\\frac{d}{dw_j} z = \\frac{d}{dw_j}(w^T x_i + b) \\Leftrightarrow \\frac{dz}{dw_j} = x_{ij} \\Leftrightarrow \\frac{d}{dw_j} = \\frac{d}{dz}x_{ij}$$\n",
    "\n",
    "Donc:\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz}x_{ij}\\sigma (z) + (1 - y_i)\\frac{1}{1 - \\sigma (z)}\\frac{d}{dz}x_{ij}(1 - \\sigma (z))$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz}\\sigma (z)x_{ij} + (1 - y_i)\\frac{1}{1 - \\sigma (z)}(-\\frac{d}{dz} \\sigma (z)x_{ij})$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\sigma (z) (1 - \\sigma (z)) x_{ij} + (1 - y_i)\\frac{1}{1 - \\sigma (z)}(- \\sigma (z))(1 - \\sigma(z))x_{ij}$$\n",
    "$$=y_i (1 - \\sigma (z)) x_{ij} - (1 - y_i)\\sigma (z)x_{ij}$$\n",
    "$$=y_i x_{ij} - y_i \\sigma (z) x_{ij} + (y_i - 1)\\sigma (z)x_{ij}$$\n",
    "$$=y_i x_{ij} + (y_i - 1 - y_i)\\sigma (z)x_{ij}$$\n",
    "$$=\\boxed{(y_i - \\sigma(z))x_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) $\\frac{dlog(p(yi|xi;w,b))}{db}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dlog(p(yi|xi;w,b))}{db}$$\n",
    "\n",
    "$$=\\frac{d}{db}(y_i log(p_i) + (1 - y_i)log(1 - p_i))$$\n",
    "$$=y_i \\frac{d}{db}log(p_i) + (1 - y_i)\\frac{d}{db}log(1 - p_i)$$\n",
    "$$=y_i \\frac{1}{p_i}\\frac{d}{db} p_i + (1 - y_i) \\frac{1}{1 - p_i} \\frac{d}{db}(1 - p_i)$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{db} \\sigma(z) + (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{db}(1 - \\sigma (z)),\\ z = w^T x_i + b$$\n",
    "\n",
    "On a:\n",
    "\n",
    "$$\\frac{d}{db} z = \\frac{d}{db}(w^T x_i + b) \\Leftrightarrow \\frac{dz}{db} = 1 \\Leftrightarrow \\frac{d}{db} = \\frac{d}{dz}$$\n",
    "\n",
    "Donc:\n",
    "$$y_i \\frac{1}{\\sigma (z)}\\frac{d}{db} \\sigma(z) + (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{db}(1 - \\sigma (z))$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz} \\sigma(z) + (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{dz}(1 - \\sigma (z))$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz} \\sigma(z) - (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{dz}\\sigma (z)$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\sigma(z) (1 - \\sigma (z)) - (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\sigma (z)(1 - \\sigma (z))$$\n",
    "$$=y_i (1 - \\sigma (z)) - (1 - y_i)\\sigma (z)$$\n",
    "$$=y_i - y_i \\sigma (z) + (y_i - 1)\\sigma (z)$$\n",
    "$$=y_i + (y_i - 1 - y_i)\\sigma (z)$$\n",
    "$$=\\boxed{y_i - \\sigma (z)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implémenter une fonction train_logistic_regression qui prend en arguments:\n",
    "\n",
    "    (a) Une matrice de covariables X\n",
    "  \n",
    "    (b) Un vecteur de labels y\n",
    "\n",
    "    (c) Un vecteur de poids initial w\n",
    "\n",
    "    (d) Une valeur de biais initiale\n",
    "\n",
    "    (e) Un nombre d’itérations num_iters\n",
    "    \n",
    "    (f) Un taux d’apprentissage learning_rate et qui renvoit les poids et le biais entraînés par descente de gradient à minimiser\n",
    "    $$−\\sum_{i = 1}^N log(p(y_i|x_i; w, b))$$\n",
    "    avec $N$ le nombre d’exemples d’entraînement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on a $−\\sum_{i = 1}^N log(p(y_i|x_i; w, b))$, alors on a notre dérivée par rapport à $dw_j$ qui nous donne: \n",
    "$$\\frac{d}{dw_j}(−\\sum_{i = 1}^N log(p(y_i|x_i; w, b)))$$\n",
    "$$= −\\sum_{i = 1}^N (y_i - \\sigma(z))x_{ij}$$\n",
    "$$=\\sum_{i = 1}^N (\\sigma(z) - y_i)x_{ij}$$\n",
    "\n",
    "Et notre dérivée par rapport à $db$ qui nous donne:\n",
    "$$\\frac{d}{db}(−\\sum_{i = 1}^N log(p(y_i|x_i; w, b)))$$\n",
    "$$= −\\sum_{i = 1}^N (y_i - \\sigma(z))$$\n",
    "$$=\\sum_{i = 1}^N (\\sigma(z) - y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter w: [0.28625294 9.95965667 5.40224131]\n",
      "Parameter b: -10.099159412529197\n",
      "\n",
      "[1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "(0.31666666666666665, 0.016666666666666666, 0.3, 0.36666666666666664)\n"
     ]
    }
   ],
   "source": [
    "# The function `sigma(z)` is implementing the sigmoid function. The sigmoid function takes in a value `z` and returns a value between 0 and 1. It is commonly used in logistic regression to map the output of a linear function to a probability value. In this case, it is used to calculate the probability of a binary outcome (0 or 1) based on the input `z`.\n",
    "def sigma(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "# The function `gradient_w` calculates the gradient of the cost function with respect to the weight vector `w` in logistic regression.\n",
    "def gradient_w(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float):\n",
    "    return (sigma(X @ w + b) - y) @ X\n",
    "\n",
    "# The function `gradient_b` calculates the gradient of the cost function with respect to the bias term `b` in logistic regression. It takes in the input matrix `X`, the target vector `y`, the weight vector `w`, and the bias term `b`.\n",
    "def gradient_b(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float):\n",
    "    return np.sum(sigma(X @ w + b) - y)\n",
    "\n",
    "# The `covariable_matrix` function is a preprocessing step in logistic regression. It takes in a numpy array `data`, an integer `target`, and a list `maximum` (with default values [1, 60, 150000]).\n",
    "def covariable_matrix(data: np.ndarray, target: int, maximum: list = [1, 60, 150000]) -> np.ndarray:\n",
    "    data[data == \"Male\"] = 1\n",
    "    data[data == \"Female\"] = 0\n",
    "    X = np.delete(data, target, 1)\n",
    "    X = X.astype(float)\n",
    "    for i in range(X.shape[1]):\n",
    "        X[:, i] /= maximum[i]\n",
    "    return X\n",
    "\n",
    "\n",
    "# The `train_logistic_regression` function is implementing the training process for logistic regression. It takes in the input matrix `X`, the target vector `y`, the weight vector `w`, the bias term `b`, the number of iterations `num_iters`, and the learning rate `learning_rate`.\n",
    "def train_logistic_regression(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, num_iters: int, learning_rate: float):\n",
    "    gradient = [w, b]\n",
    "    for i in range(num_iters):\n",
    "        gradient[0] = gradient_w(X, y, w, b)\n",
    "        gradient[1] = gradient_b(X, y, w, b)\n",
    "        w -= learning_rate * gradient[0]\n",
    "        b -= learning_rate * gradient[1]\n",
    "    return w, b\n",
    "\n",
    "# The `logistic_regression` function is implementing the logistic regression algorithm. It takes in the input data as a numpy array `data`, the index of the target variable `target`, the number of iterations `num_iters`, and the learning rate `learning_rate`.\n",
    "def logistic_regression(data: np.ndarray, target: int, num_iters: int, learning_rate: float):\n",
    "    X = covariable_matrix(data, target)\n",
    "    y = data[:, target].astype(int)\n",
    "    w = np.zeros(len(X[0]))\n",
    "    b = 0.0\n",
    "    return train_logistic_regression(X, y, w, b, num_iters, learning_rate)\n",
    "\n",
    "w, b = logistic_regression(data, 3, 1000, 0.005)\n",
    "\n",
    "print(f\"Parameter w: {w}\")\n",
    "print(f\"Parameter b: {b}\\n\")\n",
    "\n",
    "\n",
    "# The `evaluate` function is used to evaluate the logistic regression model on a given dataset. It takes in the input data as a numpy array `data`, the index of the target variable `target`, the weight vector `w`, the bias term `b`, and an optional threshold value (default is 0.5).\n",
    "def evaluate(data: np.ndarray, target: int, w: np.ndarray, b: float, threshold: float = 0.5):\n",
    "    X = covariable_matrix(data, target)\n",
    "    result = sigma(X @ w + b)\n",
    "    result[result >= threshold] = 1\n",
    "    result[result < threshold] = 0\n",
    "    return result\n",
    "\n",
    "print(evaluate(data_test, 3, w, b))\n",
    "\n",
    "# The `eval_regression` function is evaluating the performance of the logistic regression model on a given dataset. It takes in the training data (`data`), the testing data (`data_test`), the header of the dataset (`header`), the index of the target variable (`target`), the number of iterations for training (`num_iters`), and the learning rate for training (`learning_rate`).\n",
    "def eval_regression(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, num_iters: int = 1000, learning_rate: float = 0.005):\n",
    "    \"\"\"\n",
    "    Function used to evaluate the performance of the logistic regression on a given test dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data used to train the regression\n",
    "    header: np.ndarray\n",
    "        header of the data\n",
    "    data_test: np.ndarray\n",
    "        test dataset\n",
    "    target: int\n",
    "        Target column contains label\n",
    "    num_iters: int\n",
    "        used to train the regression\n",
    "    learning_rate: float\n",
    "        used to train the regression. Give the learning rate.\n",
    "\n",
    "    Return values\n",
    "    -------------\n",
    "    The probability of false negative, false positive, true negative, true positive\n",
    "    \"\"\"\n",
    "    FN_count = 0\n",
    "    FP_count = 0\n",
    "    TN_count = 0\n",
    "    TP_count = 0\n",
    "    y = data_test[:, target].astype(int)\n",
    "    w, b = logistic_regression(data, target, num_iters, learning_rate)\n",
    "    result = evaluate(data_test, target, w, b)\n",
    "    for i in range(len(y)):\n",
    "        if result[i] == 0 and y[i] == 1:\n",
    "            FN_count += 1\n",
    "        elif result[i] == 1 and y[i] == 0:\n",
    "            FP_count += 1\n",
    "        elif result[i] == 0 and y[i] == 0:\n",
    "            TN_count += 1\n",
    "        elif result[i] == 1 and y[i] == 1:\n",
    "            TP_count += 1\n",
    "    return FN_count / len(y), FP_count / len(y), TN_count / len(y), TP_count / len(y)\n",
    "\n",
    "print(eval_regression(data, data_test, header, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Evaluation\n",
    "Comparer les performances des modèles développés en les évaluant sur les données de data_train.csv\n",
    "et data_test.csv selon les métriques suivantes: accuracy, precision, recall, F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, regression: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Return the error rate of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return the probability of making an error in our prediction\n",
    "    \"\"\"\n",
    "    if regression:\n",
    "        fn, fp, tn, tp = eval_regression(data, data_test, header, target)\n",
    "    else:\n",
    "        fn, fp, tn, tp = eval_naive_base(data, data_test, header, target)\n",
    "    return (fp + fn)/(tp + tn + fp + fn)\n",
    "\n",
    "def precision(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, regression: bool = False):\n",
    "    \"\"\"\n",
    "    Return the precision of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "\n",
    "    Return value TODO !!!\n",
    "    ------------\n",
    "    Return the precision and the recall of the tree\n",
    "    \"\"\"\n",
    "    if regression:\n",
    "        fn, fp, tn, tp = eval_regression(data, data_test, header, target)\n",
    "    else:\n",
    "        fn, fp, tn, tp = eval_naive_base(data, data_test, header, target)\n",
    "    return tp / (tp + fp), tp / (tp + fn)\n",
    "\n",
    "def f1_score(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, regression: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Return the f1 score of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    Return the f1 score of the tree\n",
    "    \"\"\"\n",
    "    p, r = precision(data, data_test, header, target, regression)\n",
    "    return (2 * p * r) / (p + r)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
