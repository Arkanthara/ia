{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Travaux pratiques d’IA\n",
    "## <center> Série 5: Naive Bayes & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données\n",
    "1. Les données à utiliser pour ce TP se trouvent dans les fichiers data_train.csv et data_test csv.\n",
    "    Il s’agit de prédire l’achat d’un produit en fonction du sexe, de l’âge et du salaire d’un individu.\n",
    "2. Les 3 premières colonnes spécifient les covariables tandis que la dernière colonne correspond aux labels.\n",
    "3. Le fichier data_test.csv sert à évaluer les performances des modèles développés à partir des données du fichier data_train.csv.\n",
    "4. Il est recommandé d’utiliser pandas et/ou NumPy pour manipuler les données. Notamment la méthode get_dummies de pandas vous permet de convertir la première covariable en deux covariables binaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Naive Bayes\n",
    "Le but de cette section est d’implémenter Naive Bayes. Voici les différentes étapes à accomplir:\n",
    "\n",
    "1. Calculer la distribution empirique des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def convert_csv2array(name: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert a csv file into numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: string\n",
    "        Path to the file\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    header and data of csv\n",
    "    \"\"\"\n",
    "    file = open(name, 'r')\n",
    "    data = []\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "    data = np.array(data)\n",
    "    return data[0, :], data[1:, :]\n",
    "\n",
    "def ampiric_distribution(data: np.ndarray, target: int) -> float:\n",
    "    return np.sum(data[:, target].astype(int))/len(data)\n",
    "\n",
    "header, data = convert_csv2array('data_train.csv')\n",
    "\n",
    "# P(X == 1)\n",
    "def ampiric_distribution(data: np.ndarray, target: int) -> float:\n",
    "    return np.sum(data[:, target].astype(int))/len(data)\n",
    "\n",
    "print(ampiric_distribution(data, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pour chaque valeur des labels, estimer les paramètres des distributions des covariables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': (0.7, {'Gender': {'Female': 0.49159663865546216, 'Male': 0.5084033613445378}, 'Age': {'Mean': 32.26890756302521, 'Variance': 64.32264670574112}, 'EstimatedSalary': {'Mean': 60100.84033613445, 'Variance': 625023444.6719865}}), '1': (0.3, {'Gender': {'Female': 0.5490196078431373, 'Male': 0.45098039215686275}, 'Age': {'Mean': 45.0, 'Variance': 79.66666666666667}, 'EstimatedSalary': {'Mean': 96549.01960784313, 'Variance': 1617855440.2153018}})}\n"
     ]
    }
   ],
   "source": [
    "def distrib_param(data: np.ndarray, header: np.ndarray, target: int):\n",
    "    \"\"\"\n",
    "    Get parameters of all distributions of a set of label and covariables\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data to compute\n",
    "    header: np.ndarray\n",
    "        Header of the data\n",
    "    target: int\n",
    "        Column target for the label\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    dict contain labels, parameter of the distrib, and parameter of the distribuof each variable\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    values = np.unique(data[:, target])\n",
    "    for i in range(len(values)):\n",
    "        newdata = data[data[:, target] == values[i]]\n",
    "        tmp = {}\n",
    "        for j in range(len(data[0])):\n",
    "            if j != target:\n",
    "                tmp_2 = {}\n",
    "                newvalues, count = np.unique(newdata[:, j], return_counts = True)\n",
    "                if len(newvalues) == 2:\n",
    "                    for k in range(len(newvalues)):\n",
    "                        tmp_2[newvalues[k]] = count[k] / len(newdata)\n",
    "                else:\n",
    "                    tmp_2[\"Mean\"] = np.mean(newdata[:, j].astype(float))\n",
    "                    tmp_2[\"Variance\"] = np.var(newdata[:, j].astype(float))\n",
    "                tmp[header[j]] = tmp_2\n",
    "        result[values[i]] = (len(newdata) / len(data), tmp)\n",
    "    return result\n",
    "\n",
    "print(distrib_param(data, header, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implémenter la fonction de densité gausienne et la fonction de probabilité de Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_density_function(x: float, mean: float, var: float) -> float:\n",
    "    return (1/np.sqrt(var * 2 * np.pi)) * np.exp((-1/2)* ((x - mean)/np.sqrt(var))**2)\n",
    "\n",
    "def bernoulli_density_function(x: float, p: float) -> float:\n",
    "    return (p ** x)((1 - p) ** (1 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Étant donné de nouvelles covariables, prédire les labels correspondants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2833333333333333, 0.0, 0.31666666666666665, 0.4)\n"
     ]
    }
   ],
   "source": [
    "def naive_base(distrib: dict, header: np.ndarray, data: np.ndarray):\n",
    "    values_keys = list(distrib.keys())\n",
    "    maximum = 0\n",
    "    value = values_keys[0]\n",
    "    for i in values_keys:\n",
    "        denom = 1\n",
    "        for j in range(len(header)):\n",
    "            if list(distrib[i][1][header[j]].keys())[0] == \"Mean\" or list(distrib[i][1][header[j]].keys())[1] == \"Mean\":\n",
    "                denom *= gaussian_density_function(float(data[j]), distrib[i][1][header[j]][\"Mean\"], distrib[i][1][header[j]][\"Variance\"])\n",
    "            else:\n",
    "                denom *= distrib[i][1][header[j]][data[j]]\n",
    "\n",
    "        if denom * distrib[i][0] > maximum:\n",
    "            maximum = denom\n",
    "            value = i\n",
    "    return int(value)\n",
    "\n",
    "FN, FP, TN, TP = range(4)\n",
    "\n",
    "def test_naive_base(distrib: dict, header: np.ndarray, data: np.ndarray, target: int):\n",
    "    target_values = data[:, target].astype(int)\n",
    "    data = np.delete(data, target, 1)\n",
    "    header = np.delete(header, target)\n",
    "\n",
    "    FN_count = 0\n",
    "    FP_count = 0\n",
    "    TN_count = 0\n",
    "    TP_count = 0\n",
    "    for i in range(len(data)):\n",
    "        result = naive_base(distrib, header, data[i])\n",
    "\n",
    "        if result == 1 and target_values[i] == 1:\n",
    "            TP_count += 1\n",
    "        elif result == 0 and target_values[i] == 0:\n",
    "            TN_count += 1\n",
    "        elif result == 1 and target_values[i] == 0:\n",
    "            FP_count += 1\n",
    "        elif result == 0 and target_values[i] == 1:\n",
    "            FN_count += 1\n",
    "    return FN_count / len(data), FP_count / len(data), TN_count / len(data), TP_count / len(data)\n",
    "\n",
    "\n",
    "_, data_test = convert_csv2array('data_test.csv')\n",
    "\n",
    "print(test_naive_base(distrib_param(data, header, 3), header, data_test, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Logistic Regression\n",
    "Le but de cette section est d’implémenter Logistic Regression. On suppose donc le modèle\n",
    "suivant:\n",
    "$$yi \\approx^{ind} Bernoulli(p_i), p_i = \\sigma(w^T x_i + b), \\sigma(z) = \\frac{1}{1+exp−z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sur papier, dériver:\n",
    "\n",
    "(a) $p(y_i|x_i; w, b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y_i|x_i; w, b)$$\n",
    "$$= p_i^{y_i}(1 - p_i)^{1 - y_i}$$\n",
    "$$= \\sigma(w^T x_i + b)^{y_i}(1 - \\sigma(w^T x_i + b))^{1 - y_i}$$\n",
    "$$= (\\frac{1}{1+e^{w^T x_i + b}})^{y_i}(1 - \\frac{1}{1+e^{w^T x_i + b}})^{1 - y_i}$$\n",
    "$$= (\\frac{1}{1+e^{w^T x_i + b}})^{y_i}(\\frac{1+e^{w^T x_i + b} - 1}{1+e^{w^T x_i + b}})^{1 - y_i}$$\n",
    "$$= \\boxed{(\\frac{1}{1+e^{w^T x_i + b}})^{y_i}(\\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}})^{1 - y_i}}$$\n",
    "$$= \\frac{1}{(1+e^{w^T x_i + b})^{y_i}}\\frac{(e^{w^T x_i + b})^{1 - y_i}}{(1+e^{w^T x_i + b})^{1 - y_i}}$$\n",
    "$$= \\boxed{\\frac{(e^{w^T x_i + b})^{1 - y_i}}{1+e^{w^T x_i + b}}}$$\n",
    "\n",
    "$$p(y_i|x_i; w, b) = p_i^{y_i}(1 - p_i)^{1 - y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) $log(p(y_i|x_i; w, b))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log(p(y_i|x_i; w, b))$$\n",
    "$$= log(\\frac{(e^{w^T x_i + b})^{1 - y_i}}{1+e^{w^T x_i + b}})$$\n",
    "$$= log((e^{w^T x_i + b})^{1 - y_i}) - log({1+e^{w^T x_i + b}})$$\n",
    "$$=(1 - y_i) log(e^{w^T x_i + b}) - log({1+e^{w^T x_i + b}})$$\n",
    "$$=\\boxed{(1 - y_i) (w^T x_i + b) - log({1+e^{w^T x_i + b}})}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$=log((\\frac{1}{1+e^{w^T x_i + b}})^{y_i}(\\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}})^{1 - y_i})$$\n",
    "$$=log((\\frac{1}{1+e^{w^T x_i + b}})^{y_i}) + log((\\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}})^{1 - y_i}))$$\n",
    "$$=y_i log(\\frac{1}{1+e^{w^T x_i + b}}) + (1 - y_i) log(\\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}}))$$\n",
    "$$=-y_i log(1+e^{w^T x_i + b}) + (1 - y_i) (log(e^{w^T x_i + b}) - log(1+e^{w^T x_i + b}))$$\n",
    "$$=-y_i log(1+e^{w^T x_i + b}) + (1 - y_i) log(e^{w^T x_i + b}) + (y_i - 1) log(1+e^{w^T x_i + b})$$\n",
    "$$=(1 - y_i) log(e^{w^T x_i + b}) - log(1+e^{w^T x_i + b})$$\n",
    "\n",
    "---------------\n",
    "\n",
    "$$log(p(y_i|x_i; w, b))$$\n",
    "$$= log(p_i^{y_i}(1 - p_i)^{1 - y_i})$$\n",
    "$$= log(p_i^{y_i}) + log((1 - p_i)^{1 - y_i})$$\n",
    "$$= y_i log(p_i) + (1 - y_i)log(1 - p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) $\\frac{d\\sigma(z)}{dz}$ comme une fonction de $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d\\sigma(z)}{dz}$$\n",
    "$$= ((1 + e^{-z})^{-1})'$$\n",
    "$$= -1 \\times - e ^{-z} \\times (1 + e^{-z})^{-2}$$\n",
    "$$=\\frac{e^{-z}}{(1 + e^{-z})^2}$$\n",
    "$$=\\frac{1}{1 + e^{-z}}\\frac{e^{-z}}{1 + e^{-z}}$$\n",
    "$$= \\sigma (z) \\frac{e^{-z}}{1 + e^{-z}}$$\n",
    "$$= \\sigma (z) \\frac{1 + e^{-z} - 1}{1 + e^{-z}}$$\n",
    "$$= \\sigma (z) (\\frac{1 + e^{-z}}{1 + e^{-z}} - \\frac{1}{1 + e^{-z}})$$\n",
    "$$= \\sigma (z) (1 - \\frac{1}{1 + e^{-z}})$$\n",
    "$$= \\sigma (z) (1 - \\sigma (z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) $\\frac{dlog(p(y_i|x_i;w,b))}{dw_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dlog(p(y_i|x_i;w,b))}{dw_j}$$\n",
    "\n",
    "$$=\\frac{y_i log(p_i) + (1 - y_i)log(1 - p_i)}{dw_j}$$\n",
    "$$=y_i \\frac{log(p_i)}{dw_j} + (1 - y_i)\\frac{log(1 - p_i)}{dw_j}$$\n",
    "$$=y_i \\frac{log(\\sigma (z))}{dw_j} + (1 - y_i)\\frac{log(1 - \\sigma (z))}{dw_j},\\ z = w^T x_i + b$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dw_j}\\sigma (z) + (1 - y_i)\\frac{1}{1 - \\sigma (z)}\\frac{d}{dw_j}(1 - \\sigma (z))$$\n",
    "\n",
    "On a:\n",
    "$$\\frac{d}{dw_j} z = \\frac{d}{dw_j}(w^T x_i + b) \\Leftrightarrow \\frac{dz}{dw_j} = x_{ij} \\Leftrightarrow \\frac{d}{dw_j} = \\frac{d}{dz}x_{ij}$$\n",
    "\n",
    "Donc:\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz}x_{ij}\\sigma (z) + (1 - y_i)\\frac{1}{1 - \\sigma (z)}\\frac{d}{dz}x_{ij}(1 - \\sigma (z))$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz}\\sigma (z)x_{ij} + (1 - y_i)\\frac{1}{1 - \\sigma (z)}(-\\frac{d}{dz} \\sigma (z)x_{ij})$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\sigma (z) (1 - \\sigma (z)) x_{ij} + (1 - y_i)\\frac{1}{1 - \\sigma (z)}(- \\sigma (z))(1 - \\sigma(z))x_{ij}$$\n",
    "$$=y_i (1 - \\sigma (z)) x_{ij} - (1 - y_i)\\sigma (z)x_{ij}$$\n",
    "$$=y_i x_{ij} - y_i \\sigma (z) x_{ij} + (y_i - 1)\\sigma (z)x_{ij}$$\n",
    "$$=y_i x_{ij} + (y_i - 1 - y_i)\\sigma (z)x_{ij}$$\n",
    "$$=\\boxed{(y_i - \\sigma(z))x_{ij}}$$\n",
    "\n",
    "\n",
    "-----------------------\n",
    "$$=\\frac{(1 - y_i)(w^T x_i + b) - log(1+e^{w^T x_i + b})}{dw_j}$$\n",
    "$$= \\frac{(1 - y_i)((\\sum_{k = 0}^{N} w_k x_i) + b) - log({1+e^{\\sum_{k = 0}^{N} w_k x_i + b}})}{dw_j}$$\n",
    "$$= x_i (1 - y_i) - \\frac{log({1+e^{\\sum_{k = 0}^{N} w_k x_i + b}})}{dw_j}$$\n",
    "$$= x_i (1 - y_i) - \\frac{\\frac{1+e^{\\sum_{k = 0}^{N} w_k x_i + b}}{dw_j}}{1+e^{w^T x_i + b}}$$\n",
    "$$= \\boxed{x_i (1 - y_i) - \\frac{x_i e^{w^T x_i + b}}{1+e^{w^T x_i + b}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) $\\frac{dlog(p(yi|xi;w,b))}{db}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dlog(p(yi|xi;w,b))}{db}$$\n",
    "\n",
    "$$=\\frac{d}{db}(y_i log(p_i) + (1 - y_i)log(1 - p_i))$$\n",
    "$$=y_i \\frac{d}{db}log(p_i) + (1 - y_i)\\frac{d}{db}log(1 - p_i)$$\n",
    "$$=y_i \\frac{1}{p_i}\\frac{d}{db} p_i + (1 - y_i) \\frac{1}{1 - p_i} \\frac{d}{db}(1 - p_i)$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{db} \\sigma(z) + (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{db}(1 - \\sigma (z)),\\ z = w^T x_i + b$$\n",
    "\n",
    "On a:\n",
    "\n",
    "$$\\frac{d}{db} z = \\frac{d}{db}(w^T x_i + b) \\Leftrightarrow \\frac{dz}{db} = 1 \\Leftrightarrow \\frac{d}{db} = \\frac{d}{dz}$$\n",
    "\n",
    "Donc:\n",
    "$$y_i \\frac{1}{\\sigma (z)}\\frac{d}{db} \\sigma(z) + (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{db}(1 - \\sigma (z))$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz} \\sigma(z) + (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{dz}(1 - \\sigma (z))$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz} \\sigma(z) - (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{dz}\\sigma (z)$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\sigma(z) (1 - \\sigma (z)) - (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\sigma (z)(1 - \\sigma (z))$$\n",
    "$$=y_i (1 - \\sigma (z)) - (1 - y_i)\\sigma (z)$$\n",
    "$$=y_i - y_i \\sigma (z) + (y_i - 1)\\sigma (z)$$\n",
    "$$=y_i + (y_i - 1 - y_i)\\sigma (z)$$\n",
    "$$=\\boxed{y_i - \\sigma (z)}$$\n",
    "\n",
    "-----------\n",
    "\n",
    "$$=\\frac{(1 - y_i)(w^T x_i + b) - log(1+e^{w^T x_i + b})}{db}$$\n",
    "$$= 1 - y_i - \\frac{log(1+e^{w^T x_i + b})}{db}$$\n",
    "$$= 1 - y_i - \\frac{\\frac{1+e^{w^T x_i + b}}{db}}{1+e^{w^T x_i + b}}$$\n",
    "$$= \\boxed{1 - y_i - \\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implémenter une fonction train_logistic_regression qui prend en arguments:\n",
    "\n",
    "    (a) Une matrice de covariables X\n",
    "  \n",
    "    (b) Un vecteur de labels y\n",
    "\n",
    "    (c) Un vecteur de poids initial w\n",
    "\n",
    "    (d) Une valeur de biais initiale\n",
    "\n",
    "    (e) Un nombre d’itérations num_iters\n",
    "    \n",
    "    (f) Un taux d’apprentissage learning_rate et qui renvoit les poids et le biais entraînés par descente de gradient à minimiser\n",
    "    $$−\\sum_{i = 1}^N log(p(y_i|x_i; w, b))$$\n",
    "    avec $N$ le nombre d’exemples d’entraînement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a la formule $w^T x_i + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1.201650e+01, 7.618650e+02, 1.418324e+06]), 23.629999999999978)\n"
     ]
    }
   ],
   "source": [
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def train_logistic_regression(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, num_iters: int, learning_rate: float):\n",
    "    for i in range(num_iters):\n",
    "        dw = (y - sigma(w @ X.T + b)) @ X\n",
    "        db = np.sum(y - sigma(w @ X.T+ b))\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "    return w, b\n",
    "\n",
    "def logistic_regression(data: np.ndarray, target: int, num_iters: int, learning_rate: float):\n",
    "    X = np.delete(data, target, 1)\n",
    "    X[X == \"Male\"] = 1\n",
    "    X[X == \"Female\"] = 0\n",
    "    X = X.astype(float)\n",
    "    y = data[:, target].astype(float)\n",
    "    w = np.zeros(len(X[0]))\n",
    "    b = 0\n",
    "    return train_logistic_regression(X, y, w, b, num_iters, learning_rate)\n",
    "\n",
    "print(logistic_regression(data, 3, 100, 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     47\u001b[0m     train_logistic_regression(x, y, w, b, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mregression_logistic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m, in \u001b[0;36mregression_logistic\u001b[0;34m(data, header, target)\u001b[0m\n\u001b[1;32m     45\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;241m0\u001b[39m]))])\n\u001b[1;32m     46\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrain_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mtrain_logistic_regression\u001b[0;34m(x, y, w, b, num_iters, learning_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m result \u001b[38;5;241m=\u001b[39m [w, b]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[0;32m---> 37\u001b[0m     mygradient \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m mygradient[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     39\u001b[0m     result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m mygradient[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mgradient\u001b[0;34m(y, x, w, b)\u001b[0m\n\u001b[1;32m     26\u001b[0m result \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(w))], \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)):\n\u001b[0;32m---> 28\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtotal_derive_function_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m derive_function_b(y[i], x[i], w, b)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mT\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtotal_derive_function_w\u001b[0;34m(y, x, w, b)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_derive_function_w\u001b[39m(y: np\u001b[38;5;241m.\u001b[39mndarray, x: np\u001b[38;5;241m.\u001b[39mndarray, w: np\u001b[38;5;241m.\u001b[39mndarray, b):\n\u001b[1;32m     16\u001b[0m     result \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)):\n\u001b[1;32m     19\u001b[0m             result[i] \u001b[38;5;241m=\u001b[39m derive_function_w(y[i], x[i], w, b)\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "def function(y: float, x: float, w: np.ndarray, b):\n",
    "    return (1 - y) * np.log(w.T @ x + b) - np.log(1 + np.exp(w.T@x + b))\n",
    "\n",
    "def total_function(y: np.ndarray, x: np.ndarray, w: np.ndarray, b):\n",
    "    if len(y) != len(x):\n",
    "        print(\"Error\")\n",
    "        return\n",
    "    result = 0\n",
    "    for i in range(len(y)):\n",
    "        result -= function(y[i], x[i], w, b)\n",
    "\n",
    "def derive_function_w(y: float, x: float, w: np.ndarray, b):\n",
    "    return x * (1 - y) - (x * np.exp(w.T @ x + b))/(1 + np.exp(w.T @ x + b))\n",
    "\n",
    "def total_derive_function_w(y: np.ndarray, x: np.ndarray, w: np.ndarray, b):\n",
    "    result = w.copy()\n",
    "    for i in range(result):\n",
    "        for j in range(len(y)):\n",
    "            result[i] = derive_function_w(y[i], x[i], w, b)\n",
    "    return result\n",
    "\n",
    "def derive_function_b(y: float, x: float, w: np.ndarray, b):\n",
    "    return 1 - y - (np.exp(w.T @ x + b))/(1 + np.exp(w.T @ x + b))\n",
    "\n",
    "def gradient(y: np.ndarray, x: np.ndarray, w: np.ndarray, b):\n",
    "    result = [[0 for i in range(len(w))], 0]\n",
    "    for i in range(len(y)):\n",
    "        result[0] -= total_derive_function_w(y[i], x[i], w, b)\n",
    "        result[1] -= derive_function_b(y[i], x[i], w, b)\n",
    "    return result.T\n",
    "\n",
    "    \n",
    "\n",
    "def train_logistic_regression(x: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, num_iters: int, learning_rate):\n",
    "    result = [w, b]\n",
    "    for i in range(num_iters):\n",
    "        mygradient = gradient(y, x, result[0], result[1])\n",
    "        result[0] -= learning_rate * mygradient[0]\n",
    "        result[1] -= learning_rate * mygradient[1]\n",
    "    return result\n",
    "\n",
    "def regression_logistic(data: np.ndarray, header: np.ndarray, target: int):\n",
    "    x = np.delete(data, target, 1)\n",
    "    y = data[:, target]\n",
    "    w = np.array([0 for i in range(len(x[0]))])\n",
    "    b = 0\n",
    "    train_logistic_regression(x, y, w, b, 100, 0.1)\n",
    "\n",
    "print(regression_logistic(data, header, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Evaluation\n",
    "Comparer les performances des modèles développés en les évaluant sur les données de data_train.csv\n",
    "et data_test.csv selon les métriques suivantes: accuracy, precision, recall, F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tree: dict, test_datas: np.ndarray, header: np.ndarray, target: int) -> float:\n",
    "    \"\"\"\n",
    "    Return the error rate of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return the probability of making an error in our prediction\n",
    "    \"\"\"\n",
    "    fn, fp, tn, tp = evaluation(tree, test_datas, header, target)\n",
    "    return (fp + fn)/(tp + tn + fp + fn)\n",
    "\n",
    "def precision(tree, test_datas: np.ndarray, header: np.ndarray, target: int):\n",
    "    \"\"\"\n",
    "    Return the precision of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "\n",
    "    Return value TODO !!!\n",
    "    ------------\n",
    "    Return the precision and the recall of the tree\n",
    "    \"\"\"\n",
    "    fn, fp, tn, tp = evaluation(tree, test_datas, header, target)\n",
    "    return tp / (tp + fp), tp / (tp + fn)\n",
    "\n",
    "def f1_score(tree, test_datas: np.ndarray, header: np.ndarray, target: int) -> float:\n",
    "    \"\"\"\n",
    "    Return the f1 score of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    Return the f1 score of the tree\n",
    "    \"\"\"\n",
    "    p, r = precision(tree, test_datas, header, target)\n",
    "    return (2 * p * r) / (p + r)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
