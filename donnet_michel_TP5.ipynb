{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Travaux pratiques d’IA\n",
    "## <center> Série 5: Naive Bayes & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données\n",
    "1. Les données à utiliser pour ce TP se trouvent dans les fichiers data_train.csv et data_test csv.\n",
    "    Il s’agit de prédire l’achat d’un produit en fonction du sexe, de l’âge et du salaire d’un individu.\n",
    "2. Les 3 premières colonnes spécifient les covariables tandis que la dernière colonne correspond aux labels.\n",
    "3. Le fichier data_test.csv sert à évaluer les performances des modèles développés à partir des données du fichier data_train.csv.\n",
    "4. Il est recommandé d’utiliser pandas et/ou NumPy pour manipuler les données. Notamment la méthode get_dummies de pandas vous permet de convertir la première covariable en deux covariables binaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Naive Bayes\n",
    "Le but de cette section est d’implémenter Naive Bayes. Voici les différentes étapes à accomplir:\n",
    "\n",
    "1. Calculer la distribution empirique des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def convert_csv2array(name: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert a csv file into numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: string\n",
    "        Path to the file\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    header and data of csv\n",
    "    \"\"\"\n",
    "    file = open(name, 'r')\n",
    "    data = []\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "    data = np.array(data)\n",
    "    return data[0, :], data[1:, :]\n",
    "\n",
    "def ampiric_distribution(data: np.ndarray, target: int) -> float:\n",
    "    return np.sum(data[:, target].astype(int))/len(data)\n",
    "\n",
    "header, data = convert_csv2array('data_train.csv')\n",
    "\n",
    "# P(X == 1)\n",
    "def ampiric_distribution(data: np.ndarray, target: int) -> float:\n",
    "    return np.sum(data[:, target].astype(int))/len(data)\n",
    "\n",
    "print(ampiric_distribution(data, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pour chaque valeur des labels, estimer les paramètres des distributions des covariables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': (0.7, {'Gender': {'Female': 0.49159663865546216, 'Male': 0.5084033613445378}, 'Age': {'Mean': 32.26890756302521, 'Variance': 64.32264670574112}, 'EstimatedSalary': {'Mean': 60100.84033613445, 'Variance': 625023444.6719865}}), '1': (0.3, {'Gender': {'Female': 0.5490196078431373, 'Male': 0.45098039215686275}, 'Age': {'Mean': 45.0, 'Variance': 79.66666666666667}, 'EstimatedSalary': {'Mean': 96549.01960784313, 'Variance': 1617855440.2153018}})}\n"
     ]
    }
   ],
   "source": [
    "def distrib_param(data: np.ndarray, header: np.ndarray, target: int):\n",
    "    \"\"\"\n",
    "    Get parameters of all distributions of a set of label and covariables\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data to compute\n",
    "    header: np.ndarray\n",
    "        Header of the data\n",
    "    target: int\n",
    "        Column target for the label\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    dict contain labels, parameter of the distrib, and parameter of the distribuof each variable\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    values = np.unique(data[:, target])\n",
    "    for i in range(len(values)):\n",
    "        newdata = data[data[:, target] == values[i]]\n",
    "        tmp = {}\n",
    "        for j in range(len(data[0])):\n",
    "            if j != target:\n",
    "                tmp_2 = {}\n",
    "                newvalues, count = np.unique(newdata[:, j], return_counts = True)\n",
    "                if len(newvalues) == 2:\n",
    "                    for k in range(len(newvalues)):\n",
    "                        tmp_2[newvalues[k]] = count[k] / len(newdata)\n",
    "                else:\n",
    "                    tmp_2[\"Mean\"] = np.mean(newdata[:, j].astype(float))\n",
    "                    tmp_2[\"Variance\"] = np.var(newdata[:, j].astype(float))\n",
    "                tmp[header[j]] = tmp_2\n",
    "        result[values[i]] = (len(newdata) / len(data), tmp)\n",
    "    return result\n",
    "\n",
    "print(distrib_param(data, header, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implémenter la fonction de densité gausienne et la fonction de probabilité de Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_density_function(x: float, mean: float, var: float) -> float:\n",
    "    return (1/np.sqrt(var * 2 * np.pi)) * np.exp((-1/2)* ((x - mean)/np.sqrt(var))**2)\n",
    "\n",
    "def bernoulli_density_function(x: float, p: float) -> float:\n",
    "    return (p ** x)((1 - p) ** (1 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Étant donné de nouvelles covariables, prédire les labels correspondants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2833333333333333, 0.0, 0.31666666666666665, 0.4)\n"
     ]
    }
   ],
   "source": [
    "def naive_base(distrib: dict, header: np.ndarray, data: np.ndarray):\n",
    "    values_keys = list(distrib.keys())\n",
    "    maximum = 0\n",
    "    value = values_keys[0]\n",
    "    for i in values_keys:\n",
    "        denom = 1\n",
    "        for j in range(len(header)):\n",
    "            if list(distrib[i][1][header[j]].keys())[0] == \"Mean\" or list(distrib[i][1][header[j]].keys())[1] == \"Mean\":\n",
    "                denom *= gaussian_density_function(float(data[j]), distrib[i][1][header[j]][\"Mean\"], distrib[i][1][header[j]][\"Variance\"])\n",
    "            else:\n",
    "                denom *= distrib[i][1][header[j]][data[j]]\n",
    "\n",
    "        if denom * distrib[i][0] > maximum:\n",
    "            maximum = denom\n",
    "            value = i\n",
    "    return int(value)\n",
    "\n",
    "FN, FP, TN, TP = range(4)\n",
    "\n",
    "def test_naive_base(distrib: dict, header: np.ndarray, data: np.ndarray, target: int):\n",
    "    target_values = data[:, target].astype(int)\n",
    "    data = np.delete(data, target, 1)\n",
    "    header = np.delete(header, target)\n",
    "\n",
    "    FN_count = 0\n",
    "    FP_count = 0\n",
    "    TN_count = 0\n",
    "    TP_count = 0\n",
    "    for i in range(len(data)):\n",
    "        result = naive_base(distrib, header, data[i])\n",
    "\n",
    "        if result == 1 and target_values[i] == 1:\n",
    "            TP_count += 1\n",
    "        elif result == 0 and target_values[i] == 0:\n",
    "            TN_count += 1\n",
    "        elif result == 1 and target_values[i] == 0:\n",
    "            FP_count += 1\n",
    "        elif result == 0 and target_values[i] == 1:\n",
    "            FN_count += 1\n",
    "    return FN_count / len(data), FP_count / len(data), TN_count / len(data), TP_count / len(data)\n",
    "\n",
    "\n",
    "_, data_test = convert_csv2array('data_test.csv')\n",
    "\n",
    "print(test_naive_base(distrib_param(data, header, 3), header, data_test, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Logistic Regression\n",
    "Le but de cette section est d’implémenter Logistic Regression. On suppose donc le modèle\n",
    "suivant:\n",
    "$$yi \\approx^{ind} Bernoulli(p_i), p_i = \\sigma(w^T x_i + b), \\sigma(z) = \\frac{1}{1+exp−z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sur papier, dériver:\n",
    "\n",
    "(a) $p(y_i|x_i; w, b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y_i|x_i; w, b)$$\n",
    "$$= p_i^{y_i}(1 - p_i)^{1 - y_i}$$\n",
    "$$= \\sigma(w^T x_i + b)^{y_i}(1 - \\sigma(w^T x_i + b))^{1 - y_i}$$\n",
    "$$= (\\frac{1}{1+e^{w^T x_i + b}})^{y_i}(1 - \\frac{1}{1+e^{w^T x_i + b}})^{1 - y_i}$$\n",
    "$$= (\\frac{1}{1+e^{w^T x_i + b}})^{y_i}(\\frac{1+e^{w^T x_i + b} - 1}{1+e^{w^T x_i + b}})^{1 - y_i}$$\n",
    "$$= \\boxed{(\\frac{1}{1+e^{w^T x_i + b}})^{y_i}(\\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}})^{1 - y_i}}$$\n",
    "$$= \\frac{1}{(1+e^{w^T x_i + b})^{y_i}}\\frac{(e^{w^T x_i + b})^{1 - y_i}}{(1+e^{w^T x_i + b})^{1 - y_i}}$$\n",
    "$$= \\boxed{\\frac{(e^{w^T x_i + b})^{1 - y_i}}{1+e^{w^T x_i + b}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) $log(p(y_i|x_i; w, b))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log(p(y_i|x_i; w, b))$$\n",
    "$$= log(\\frac{(e^{w^T x_i + b})^{1 - y_i}}{1+e^{w^T x_i + b}})$$\n",
    "$$= log((e^{w^T x_i + b})^{1 - y_i}) - log({1+e^{w^T x_i + b}})$$\n",
    "$$=(1 - y_i) log(e^{w^T x_i + b}) - log({1+e^{w^T x_i + b}})$$\n",
    "$$=\\boxed{(1 - y_i) (w^T x_i + b) - log({1+e^{w^T x_i + b}})}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$=log((\\frac{1}{1+e^{w^T x_i + b}})^{y_i}(\\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}})^{1 - y_i})$$\n",
    "$$=log((\\frac{1}{1+e^{w^T x_i + b}})^{y_i}) + log((\\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}})^{1 - y_i}))$$\n",
    "$$=y_i log(\\frac{1}{1+e^{w^T x_i + b}}) + (1 - y_i) log(\\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}}))$$\n",
    "$$=-y_i log(1+e^{w^T x_i + b}) + (1 - y_i) (log(e^{w^T x_i + b}) - log(1+e^{w^T x_i + b}))$$\n",
    "$$=-y_i log(1+e^{w^T x_i + b}) + (1 - y_i) log(e^{w^T x_i + b}) + (y_i - 1) log(1+e^{w^T x_i + b})$$\n",
    "$$=(1 - y_i) log(e^{w^T x_i + b}) - log(1+e^{w^T x_i + b})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) $\\frac{d\\sigma(z)}{dz}$ comme une fonction de $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d\\sigma(z)}{dz}$$\n",
    "$$= ((1 + e^{-z})^{-1})'$$\n",
    "$$= -1 \\times - e ^{-z} \\times (1 + e^{-z})^{-2}$$\n",
    "$$=\\frac{e^{-z}}{(1 + e^{-z})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) $\\frac{dlog(p(y_i|x_i;w,b))}{dw_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dlog(p(y_i|x_i;w,b))}{dw_j}$$\n",
    "\n",
    "$$=\\frac{(1 - y_i)(w^T x_i + b) - log(1+e^{w^T x_i + b})}{dw_j}$$\n",
    "$$= \\frac{(1 - y_i)((\\sum_{k = 0}^{N} w_k x_i) + b) - log({1+e^{(\\sum_{k = 0}^{N} w_k x_i) + b}})}{dw_j}$$\n",
    "$$= x_i (1 - y_i) - \\frac{log({1+e^{(\\sum_{k = 0}^{N} w_k x_i) + b}})}{dw_j}$$\n",
    "$$= x_i (1 - y_i) - \\frac{\\frac{(1+e^{(\\sum_{k = 0}^{N} w_k x_i) + b})}{dw_j}}{1+e^{w^T x_i + b}}$$\n",
    "$$= \\boxed{x_i (1 - y_i) - \\frac{x_i e^{w^T x_i + b}}{1+e^{w^T x_i + b}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) $\\frac{dlog(p(yi|xi;w,b))}{db}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dlog(p(yi|xi;w,b))}{db}$$\n",
    "$$=\\frac{(1 - y_i)(w^T x_i + b) - log(1+e^{w^T x_i + b})}{db}$$\n",
    "$$= 1 - y_i - \\frac{log(1+e^{w^T x_i + b})}{db}$$\n",
    "$$= 1 - y_i - \\frac{\\frac{1+e^{w^T x_i + b}}{db}}{1+e^{w^T x_i + b}}$$\n",
    "$$= \\boxed{1 - y_i - \\frac{e^{w^T x_i + b}}{1+e^{w^T x_i + b}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implémenter une fonction train_logistic_regression qui prend en arguments:\n",
    "\n",
    "    (a) Une matrice de covariables X\n",
    "  \n",
    "    (b) Un vecteur de labels y\n",
    "\n",
    "    (c) Un vecteur de poids initial w\n",
    "\n",
    "    (d) Une valeur de biais initiale\n",
    "\n",
    "    (e) Un nombre d’itérations num_iters\n",
    "    \n",
    "    (f) Un taux d’apprentissage learning_rate et qui renvoit les poids et le biais entraînés par descente de gradient à minimiser\n",
    "    $$−\\sum_{i = 1}^N log(p(y_i|x_i; w, b))$$\n",
    "    avec $N$ le nombre d’exemples d’entraînement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Evaluation\n",
    "Comparer les performances des modèles développés en les évaluant sur les données de data_train.csv\n",
    "et data_test.csv selon les métriques suivantes: accuracy, precision, recall, F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tree: dict, test_datas: np.ndarray, header: np.ndarray, target: int) -> float:\n",
    "    \"\"\"\n",
    "    Return the error rate of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return the probability of making an error in our prediction\n",
    "    \"\"\"\n",
    "    fn, fp, tn, tp = evaluation(tree, test_datas, header, target)\n",
    "    return (fp + fn)/(tp + tn + fp + fn)\n",
    "\n",
    "def precision(tree, test_datas: np.ndarray, header: np.ndarray, target: int):\n",
    "    \"\"\"\n",
    "    Return the precision of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "\n",
    "    Return value TODO !!!\n",
    "    ------------\n",
    "    Return the precision and the recall of the tree\n",
    "    \"\"\"\n",
    "    fn, fp, tn, tp = evaluation(tree, test_datas, header, target)\n",
    "    return tp / (tp + fp), tp / (tp + fn)\n",
    "\n",
    "def f1_score(tree, test_datas: np.ndarray, header: np.ndarray, target: int) -> float:\n",
    "    \"\"\"\n",
    "    Return the f1 score of the decision tree for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree: dictionary\n",
    "        The decision tree\n",
    "    test_datas: np.ndarray\n",
    "        The set of tests data\n",
    "    header: np.ndarray\n",
    "        Header of test datas\n",
    "    target: int\n",
    "        target column of decision value of test datas\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    Return the f1 score of the tree\n",
    "    \"\"\"\n",
    "    p, r = precision(tree, test_datas, header, target)\n",
    "    return (2 * p * r) / (p + r)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
