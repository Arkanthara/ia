{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Travaux pratiques d’IA\n",
    "## <center> Série 5: Naive Bayes & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données\n",
    "1. Les données à utiliser pour ce TP se trouvent dans les fichiers data_train.csv et data_test csv.\n",
    "    Il s’agit de prédire l’achat d’un produit en fonction du sexe, de l’âge et du salaire d’un individu.\n",
    "2. Les 3 premières colonnes spécifient les covariables tandis que la dernière colonne correspond aux labels.\n",
    "3. Le fichier data_test.csv sert à évaluer les performances des modèles développés à partir des données du fichier data_train.csv.\n",
    "4. Il est recommandé d’utiliser pandas et/ou NumPy pour manipuler les données. Notamment la méthode get_dummies de pandas vous permet de convertir la première covariable en deux covariables binaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Naive Bayes\n",
    "Le but de cette section est d’implémenter Naive Bayes. Voici les différentes étapes à accomplir:\n",
    "\n",
    "1. Calculer la distribution empirique des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def convert_csv2array(name: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert a csv file into numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: string\n",
    "        Path to the file\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    header and data of csv\n",
    "    \"\"\"\n",
    "    file = open(name, 'r')\n",
    "    data = []\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "    data = np.array(data)\n",
    "    return data[0, :], data[1:, :]\n",
    "\n",
    "header, data = convert_csv2array('data_train.csv')\n",
    "\n",
    "def ampiric_distribution(data: np.ndarray, target: int) -> float:\n",
    "    \"\"\"\n",
    "    Get parameter of bernoulli distribution from a set of values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        set of values\n",
    "    target: int\n",
    "        Column target to compute ampiric distribution\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    The parameter p of the bernoulli distribution\n",
    "    \"\"\"\n",
    "    return np.sum(data[:, target].astype(int))/len(data)\n",
    "\n",
    "print(ampiric_distribution(data, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pour chaque valeur des labels, estimer les paramètres des distributions des covariables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': (0.7, {'Gender': {'Female': 0.49159663865546216, 'Male': 0.5084033613445378}, 'Age': {'Mean': 32.26890756302521, 'Variance': 64.32264670574112}, 'EstimatedSalary': {'Mean': 60100.84033613445, 'Variance': 625023444.6719865}}), '1': (0.3, {'Gender': {'Female': 0.5490196078431373, 'Male': 0.45098039215686275}, 'Age': {'Mean': 45.0, 'Variance': 79.66666666666667}, 'EstimatedSalary': {'Mean': 96549.01960784313, 'Variance': 1617855440.2153018}})}\n"
     ]
    }
   ],
   "source": [
    "def distrib_param(data: np.ndarray, header: np.ndarray, target: int):\n",
    "    \"\"\"\n",
    "    Get parameters of all distributions of a set of label and covariables\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data to compute\n",
    "    header: np.ndarray\n",
    "        Header of the data\n",
    "    target: int\n",
    "        Column target for the label\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    dict contain labels with parameter of the distribution of the labels, and parameter of the distributions of each variable\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    values = np.unique(data[:, target])\n",
    "    for i in range(len(values)):\n",
    "        newdata = data[data[:, target] == values[i]]\n",
    "        tmp = {}\n",
    "        for j in range(len(data[0])):\n",
    "            if j != target:\n",
    "                tmp_2 = {}\n",
    "                newvalues, count = np.unique(newdata[:, j], return_counts = True)\n",
    "                if len(newvalues) == 2:\n",
    "                    for k in range(len(newvalues)):\n",
    "                        tmp_2[newvalues[k]] = count[k] / len(newdata)\n",
    "                else:\n",
    "                    tmp_2[\"Mean\"] = np.mean(newdata[:, j].astype(float))\n",
    "                    tmp_2[\"Variance\"] = np.var(newdata[:, j].astype(float))\n",
    "                tmp[header[j]] = tmp_2\n",
    "        result[values[i]] = (len(newdata) / len(data), tmp)\n",
    "    return result\n",
    "\n",
    "print(distrib_param(data, header, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implémenter la fonction de densité gausienne et la fonction de probabilité de Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_density_function(x: float, mean: float, var: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the probability of x for a given gaussian density function, obtained thanks to mean and variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: float\n",
    "        value given to the pdf\n",
    "    mean: float\n",
    "        mean of the gaussian distribution\n",
    "    var: float\n",
    "        variance of the gaussian distribution\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    The probability of x\n",
    "    \"\"\"\n",
    "    return (1/np.sqrt(var * 2 * np.pi)) * np.exp((-1/2)* ((x - mean)/np.sqrt(var))**2)\n",
    "\n",
    "def bernoulli_density_function(x: float, p: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the probability of x for a given bernoulli density function, obtained thanks to p parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: float\n",
    "        value given to the pdf\n",
    "    p: float\n",
    "        parameter of the bernoulli distribution\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    The probability of x\n",
    "    \"\"\"\n",
    "    return (p ** x)((1 - p) ** (1 - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Étant donné de nouvelles covariables, prédire les labels correspondants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def naive_base(distrib: dict, header: np.ndarray, data: np.ndarray):\n",
    "    \"\"\"\n",
    "    This function is a naive Bayes classifier and give the result of the estimation for a data given\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distrib: dict\n",
    "        Parameter of all distributions obtained thanks to a set of data\n",
    "    header: np.ndarray\n",
    "        header of the data\n",
    "    data: np.ndarray\n",
    "        data to estimate\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    The estimation of the given data    \n",
    "    \"\"\"\n",
    "    values_keys = list(distrib.keys())\n",
    "    maximum = 0\n",
    "    value = values_keys[0]\n",
    "    for i in values_keys:\n",
    "        denom = 1\n",
    "        for j in range(len(header)):\n",
    "            if list(distrib[i][1][header[j]].keys())[0] == \"Mean\" or list(distrib[i][1][header[j]].keys())[1] == \"Mean\":\n",
    "                denom *= gaussian_density_function(float(data[j]), distrib[i][1][header[j]][\"Mean\"], distrib[i][1][header[j]][\"Variance\"])\n",
    "            else:\n",
    "                denom *= distrib[i][1][header[j]][data[j]]\n",
    "\n",
    "        if denom * distrib[i][0] > maximum:\n",
    "            maximum = denom\n",
    "            value = i\n",
    "    return int(value)\n",
    "\n",
    "FN, FP, TN, TP = range(4)\n",
    "\n",
    "\n",
    "def eval_naive_base(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, prediction: bool = False):\n",
    "    \"\"\"\n",
    "    Function used to evaluate the performance of the naive base classifier on a given test dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data used to train the classifier\n",
    "    header: np.ndarray\n",
    "        header of the data\n",
    "    data_test: np.ndarray\n",
    "        test dataset\n",
    "    target: int\n",
    "        Target column contains label\n",
    "    prediction: bool\n",
    "        Indicate if we must return performance of naive base classifier or prediction obtained for each data\n",
    "\n",
    "    Return values\n",
    "    -------------\n",
    "    The probability of false negative, false positive, true negative, true positive, or the prediction of each data\n",
    "    \"\"\"\n",
    "    target_values = data_test[:, target].astype(int)\n",
    "    data_test = np.delete(data_test, target, 1)\n",
    "    header_test = np.delete(header, target)\n",
    "\n",
    "    FN_count = 0\n",
    "    FP_count = 0\n",
    "    TN_count = 0\n",
    "    TP_count = 0\n",
    "\n",
    "    predict = []\n",
    "    for i in range(len(data_test)):\n",
    "        result = naive_base(distrib_param(data, header, target), header_test, data_test[i])\n",
    "\n",
    "        predict.append(result)\n",
    "\n",
    "        if result == 1 and target_values[i] == 1:\n",
    "            TP_count += 1\n",
    "        elif result == 0 and target_values[i] == 0:\n",
    "            TN_count += 1\n",
    "        elif result == 1 and target_values[i] == 0:\n",
    "            FP_count += 1\n",
    "        elif result == 0 and target_values[i] == 1:\n",
    "            FN_count += 1\n",
    "\n",
    "    if prediction:\n",
    "        return predict\n",
    "    \n",
    "    return FN_count / len(data_test), FP_count / len(data_test), TN_count / len(data_test), TP_count / len(data_test)\n",
    "\n",
    "\n",
    "_, data_test = convert_csv2array('data_test.csv')\n",
    "\n",
    "print(eval_naive_base(data, data_test, header, 3, True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Logistic Regression\n",
    "Le but de cette section est d’implémenter Logistic Regression. On suppose donc le modèle\n",
    "suivant:\n",
    "$$yi \\approx^{ind} Bernoulli(p_i), p_i = \\sigma(w^T x_i + b), \\sigma(z) = \\frac{1}{1+exp−z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sur papier, dériver:\n",
    "\n",
    "(a) $p(y_i|x_i; w, b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y_i|x_i; w, b) = \\boxed{p_i^{y_i}(1 - p_i)^{1 - y_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) $log(p(y_i|x_i; w, b))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log(p(y_i|x_i; w, b))$$\n",
    "$$= log(p_i^{y_i}(1 - p_i)^{1 - y_i})$$\n",
    "$$= log(p_i^{y_i}) + log((1 - p_i)^{1 - y_i})$$\n",
    "$$= \\boxed{y_i log(p_i) + (1 - y_i)log(1 - p_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) $\\frac{d\\sigma(z)}{dz}$ comme une fonction de $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d\\sigma(z)}{dz}$$\n",
    "$$= ((1 + e^{-z})^{-1})'$$\n",
    "$$= -1 \\times - e ^{-z} \\times (1 + e^{-z})^{-2}$$\n",
    "$$=\\frac{e^{-z}}{(1 + e^{-z})^2}$$\n",
    "$$=\\frac{1}{1 + e^{-z}}\\frac{e^{-z}}{1 + e^{-z}}$$\n",
    "$$= \\sigma (z) \\frac{e^{-z}}{1 + e^{-z}}$$\n",
    "$$= \\sigma (z) \\frac{1 + e^{-z} - 1}{1 + e^{-z}}$$\n",
    "$$= \\sigma (z) (\\frac{1 + e^{-z}}{1 + e^{-z}} - \\frac{1}{1 + e^{-z}})$$\n",
    "$$= \\sigma (z) (1 - \\frac{1}{1 + e^{-z}})$$\n",
    "$$= \\boxed{\\sigma (z) (1 - \\sigma (z))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) $\\frac{dlog(p(y_i|x_i;w,b))}{dw_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dlog(p(y_i|x_i;w,b))}{dw_j}$$\n",
    "\n",
    "$$=\\frac{y_i log(p_i) + (1 - y_i)log(1 - p_i)}{dw_j}$$\n",
    "$$=y_i \\frac{log(p_i)}{dw_j} + (1 - y_i)\\frac{log(1 - p_i)}{dw_j}$$\n",
    "$$=y_i \\frac{log(\\sigma (z))}{dw_j} + (1 - y_i)\\frac{log(1 - \\sigma (z))}{dw_j},\\ z = w^T x_i + b$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dw_j}\\sigma (z) + (1 - y_i)\\frac{1}{1 - \\sigma (z)}\\frac{d}{dw_j}(1 - \\sigma (z))$$\n",
    "\n",
    "On a:\n",
    "$$\\frac{d}{dw_j} z = \\frac{d}{dw_j}(w^T x_i + b) \\Leftrightarrow \\frac{dz}{dw_j} = x_{ij} \\Leftrightarrow \\frac{d}{dw_j} = \\frac{d}{dz}x_{ij}$$\n",
    "\n",
    "Donc:\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz}x_{ij}\\sigma (z) + (1 - y_i)\\frac{1}{1 - \\sigma (z)}\\frac{d}{dz}x_{ij}(1 - \\sigma (z))$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz}\\sigma (z)x_{ij} + (1 - y_i)\\frac{1}{1 - \\sigma (z)}(-\\frac{d}{dz} \\sigma (z)x_{ij})$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\sigma (z) (1 - \\sigma (z)) x_{ij} + (1 - y_i)\\frac{1}{1 - \\sigma (z)}(- \\sigma (z))(1 - \\sigma(z))x_{ij}$$\n",
    "$$=y_i (1 - \\sigma (z)) x_{ij} - (1 - y_i)\\sigma (z)x_{ij}$$\n",
    "$$=y_i x_{ij} - y_i \\sigma (z) x_{ij} + (y_i - 1)\\sigma (z)x_{ij}$$\n",
    "$$=y_i x_{ij} + (y_i - 1 - y_i)\\sigma (z)x_{ij}$$\n",
    "$$=\\boxed{(y_i - \\sigma(z))x_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) $\\frac{dlog(p(yi|xi;w,b))}{db}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dlog(p(yi|xi;w,b))}{db}$$\n",
    "\n",
    "$$=\\frac{d}{db}(y_i log(p_i) + (1 - y_i)log(1 - p_i))$$\n",
    "$$=y_i \\frac{d}{db}log(p_i) + (1 - y_i)\\frac{d}{db}log(1 - p_i)$$\n",
    "$$=y_i \\frac{1}{p_i}\\frac{d}{db} p_i + (1 - y_i) \\frac{1}{1 - p_i} \\frac{d}{db}(1 - p_i)$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{db} \\sigma(z) + (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{db}(1 - \\sigma (z)),\\ z = w^T x_i + b$$\n",
    "\n",
    "On a:\n",
    "\n",
    "$$\\frac{d}{db} z = \\frac{d}{db}(w^T x_i + b) \\Leftrightarrow \\frac{dz}{db} = 1 \\Leftrightarrow \\frac{d}{db} = \\frac{d}{dz}$$\n",
    "\n",
    "Donc:\n",
    "$$y_i \\frac{1}{\\sigma (z)}\\frac{d}{db} \\sigma(z) + (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{db}(1 - \\sigma (z))$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz} \\sigma(z) + (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{dz}(1 - \\sigma (z))$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\frac{d}{dz} \\sigma(z) - (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\frac{d}{dz}\\sigma (z)$$\n",
    "$$=y_i \\frac{1}{\\sigma (z)}\\sigma(z) (1 - \\sigma (z)) - (1 - y_i) \\frac{1}{1 - \\sigma (z)} \\sigma (z)(1 - \\sigma (z))$$\n",
    "$$=y_i (1 - \\sigma (z)) - (1 - y_i)\\sigma (z)$$\n",
    "$$=y_i - y_i \\sigma (z) + (y_i - 1)\\sigma (z)$$\n",
    "$$=y_i + (y_i - 1 - y_i)\\sigma (z)$$\n",
    "$$=\\boxed{y_i - \\sigma (z)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implémenter une fonction train_logistic_regression qui prend en arguments:\n",
    "\n",
    "    (a) Une matrice de covariables X\n",
    "  \n",
    "    (b) Un vecteur de labels y\n",
    "\n",
    "    (c) Un vecteur de poids initial w\n",
    "\n",
    "    (d) Une valeur de biais initiale\n",
    "\n",
    "    (e) Un nombre d’itérations num_iters\n",
    "    \n",
    "    (f) Un taux d’apprentissage learning_rate et qui renvoit les poids et le biais entraînés par descente de gradient à minimiser\n",
    "    $$−\\sum_{i = 1}^N log(p(y_i|x_i; w, b))$$\n",
    "    avec $N$ le nombre d’exemples d’entraînement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on a $−\\sum_{i = 1}^N log(p(y_i|x_i; w, b))$, alors on a notre dérivée par rapport à $dw_j$ qui nous donne: \n",
    "$$\\frac{d}{dw_j}(−\\sum_{i = 1}^N log(p(y_i|x_i; w, b)))$$\n",
    "$$= −\\sum_{i = 1}^N (y_i - \\sigma(z))x_{ij}$$\n",
    "$$=\\sum_{i = 1}^N (\\sigma(z) - y_i)x_{ij}$$\n",
    "\n",
    "Et notre dérivée par rapport à $db$ qui nous donne:\n",
    "$$\\frac{d}{db}(−\\sum_{i = 1}^N log(p(y_i|x_i; w, b)))$$\n",
    "$$= −\\sum_{i = 1}^N (y_i - \\sigma(z))$$\n",
    "$$=\\sum_{i = 1}^N (\\sigma(z) - y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter w: [0.28625294 9.95965667 5.40224131]\n",
      "Parameter b: -10.099159412529197\n",
      "\n",
      "[1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "(0.4166666666666667, 0.0, 0.31666666666666665, 0.26666666666666666)\n"
     ]
    }
   ],
   "source": [
    "def sigma(z):\n",
    "    \"\"\"\n",
    "    This function implements the sigmoid function.\n",
    "    The sigmoid function takes the value z and return a value between 0 and 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z:\n",
    "        value given to the function\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    return a value between 0 and 1\n",
    "    \"\"\"\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def gradient_w(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float):\n",
    "    \"\"\"\n",
    "    This function calculate the derivate compared to the weights w of the function to minimise, given above\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        Matrix of covariables\n",
    "    y: np.ndarray\n",
    "        Vector of labels\n",
    "    w: np.ndarray\n",
    "        Vector of initial weights\n",
    "    b: float\n",
    "        Value of initial bias\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    return the result of the derivate compared to the weights w of the function to minimise.\n",
    "    \"\"\"\n",
    "    return (sigma(X @ w + b) - y) @ X\n",
    "\n",
    "def gradient_b(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float):\n",
    "    \"\"\"\n",
    "    This function calculate the derivate compared to the bias b of the function to minimise, given above\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        Matrix of covariables\n",
    "    y: np.ndarray\n",
    "        Vector of labels\n",
    "    w: np.ndarray\n",
    "        Vector of initial weights\n",
    "    b: float\n",
    "        Value of initial bias\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    return the result of the derivate compared to the bias b of the function to minimise.\n",
    "    \"\"\"\n",
    "    return np.sum(sigma(X @ w + b) - y)\n",
    "\n",
    "def covariable_matrix(data: np.ndarray, target: int, maximum: list = [1, 60, 150000]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function is a preprocessing step in logistic regression allowing to have all the values in the same order of size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data to use to compute covariable matrix\n",
    "    target: int\n",
    "        target of the label column\n",
    "    maximum: list\n",
    "        List of maximum of each column to normalize the covariable matrix\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    A covariable matrix with all the values in the same order of size\n",
    "    \"\"\"\n",
    "    data[data == \"Male\"] = 1\n",
    "    data[data == \"Female\"] = 0\n",
    "    X = np.delete(data, target, 1)\n",
    "    X = X.astype(float)\n",
    "    for i in range(X.shape[1]):\n",
    "        X[:, i] /= maximum[i]\n",
    "    return X\n",
    "\n",
    "\n",
    "def train_logistic_regression(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, num_iters: int, learning_rate: float):\n",
    "    \"\"\"\n",
    "    This function implements the training process for logistic regression. It compute the optimal parameters w and b thanks to a gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        Matrix of covariables\n",
    "    y: np.ndarray\n",
    "        Vector of labels\n",
    "    w: np.ndarray\n",
    "        Vector of initial weights\n",
    "    b: float\n",
    "        Value of initial bias\n",
    "    num_iters: int\n",
    "        Number of iterations\n",
    "    learning_rate: float\n",
    "        define how the function will converge. Too big values give bad results and too smal values not converge or converge too slowly\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Optimal parameters w and b for the initial configuration and parameters\n",
    "    \"\"\"\n",
    "    gradient = [w, b]\n",
    "    for i in range(num_iters):\n",
    "        gradient[0] = gradient_w(X, y, w, b)\n",
    "        gradient[1] = gradient_b(X, y, w, b)\n",
    "        w -= learning_rate * gradient[0]\n",
    "        b -= learning_rate * gradient[1]\n",
    "    return w, b\n",
    "\n",
    "def logistic_regression(data: np.ndarray, target: int, num_iters: int, learning_rate: float):\n",
    "    \"\"\"\n",
    "    This function is used to convert data to parameters to the function above\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Data used to get optimal parameters w and b\n",
    "    target: int\n",
    "        Target column of label\n",
    "    num_iters: int\n",
    "        Number of iterations\n",
    "    learning_rate: float\n",
    "        define how the function will converge. Too big values give bad results and too smal values not converge or converge too slowly\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Optimal parameters w and b for the initial configuration and parameters\n",
    "    \"\"\"\n",
    "    X = covariable_matrix(data, target)\n",
    "    y = data[:, target].astype(int)\n",
    "    w = np.zeros(len(X[0]))\n",
    "    b = 0.0\n",
    "    return train_logistic_regression(X, y, w, b, num_iters, learning_rate)\n",
    "\n",
    "w, b = logistic_regression(data, 3, 1000, 0.005)\n",
    "\n",
    "print(f\"Parameter w: {w}\")\n",
    "print(f\"Parameter b: {b}\\n\")\n",
    "\n",
    "def evaluate(data: np.ndarray, target: int, w: np.ndarray, b: float, threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate the logistic regression model on a given dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        dataset to evaluate\n",
    "    target: int\n",
    "        target column of label\n",
    "    w: np.ndarray\n",
    "        Vector of initial weights\n",
    "    b: float\n",
    "        Value of initial bias\n",
    "    threshold: float\n",
    "        All values greater than threshold will have a label of 1 and all values less than threshold will have a label of 0\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return a set of labels of estimated data\n",
    "    \"\"\"\n",
    "    X = covariable_matrix(data, target)\n",
    "    result = sigma(X @ w + b)\n",
    "    result[result >= threshold] = 1\n",
    "    result[result < threshold] = 0\n",
    "    return result\n",
    "\n",
    "print(evaluate(data_test, 3, w, b))\n",
    "\n",
    "def eval_regression(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, num_iters: int = 1000, learning_rate: float = 0.001):\n",
    "    \"\"\"\n",
    "    Function used to evaluate the performance of the logistic regression on a given test dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data used to train the regression\n",
    "    data_test: np.ndarray\n",
    "        test dataset\n",
    "    header: np.ndarray\n",
    "        header of the data\n",
    "    target: int\n",
    "        Target column contains label\n",
    "    num_iters: int\n",
    "        used to train the regression\n",
    "    learning_rate: float\n",
    "        used to train the regression. Give the learning rate.\n",
    "\n",
    "    Return values\n",
    "    -------------\n",
    "    The probability of false negative, false positive, true negative, true positive\n",
    "    \"\"\"\n",
    "    FN_count = 0\n",
    "    FP_count = 0\n",
    "    TN_count = 0\n",
    "    TP_count = 0\n",
    "    y = data_test[:, target].astype(int)\n",
    "    w, b = logistic_regression(data, target, num_iters, learning_rate)\n",
    "    result = evaluate(data_test, target, w, b)\n",
    "    for i in range(len(y)):\n",
    "        if result[i] == 0 and y[i] == 1:\n",
    "            FN_count += 1\n",
    "        elif result[i] == 1 and y[i] == 0:\n",
    "            FP_count += 1\n",
    "        elif result[i] == 0 and y[i] == 0:\n",
    "            TN_count += 1\n",
    "        elif result[i] == 1 and y[i] == 1:\n",
    "            TP_count += 1\n",
    "    return FN_count / len(y), FP_count / len(y), TN_count / len(y), TP_count / len(y)\n",
    "\n",
    "print(eval_regression(data, data_test, header, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Evaluation\n",
    "Comparer les performances des modèles développés en les évaluant sur les données de data_train.csv\n",
    "et data_test.csv selon les métriques suivantes: accuracy, precision, recall, F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, regression: bool = False, num_iters: int = 1000, learning_rate: float = 0.005) -> float:\n",
    "    \"\"\"\n",
    "    Return the error rate of the model for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data used to train the regression\n",
    "    data_test: np.ndarray\n",
    "        test dataset\n",
    "    header: np.ndarray\n",
    "        header of the data\n",
    "    target: int\n",
    "        Target column contains label\n",
    "    regression: bool\n",
    "        Indicate if we evaluate the regression or the naive base model\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return the probability of making an error in our prediction\n",
    "    \"\"\"\n",
    "    if regression:\n",
    "        fn, fp, tn, tp = eval_regression(data, data_test, header, target, num_iters, learning_rate)\n",
    "    else:\n",
    "        fn, fp, tn, tp = eval_naive_base(data, data_test, header, target)\n",
    "    return (fp + fn)/(tp + tn + fp + fn)\n",
    "\n",
    "def precision(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, regression: bool = False, num_iters: int = 1000, learning_rate: float = 0.005):\n",
    "    \"\"\"\n",
    "    Return the precision of the model for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data used to train the regression\n",
    "    data_test: np.ndarray\n",
    "        test dataset\n",
    "    header: np.ndarray\n",
    "        header of the data\n",
    "    target: int\n",
    "        Target column contains label\n",
    "    regression: bool\n",
    "        Indicate if we evaluate the regression or the naive base model\n",
    "    num_iters: int\n",
    "        used to train the regression\n",
    "    learning_rate: float\n",
    "        used to train the regression. Give the learning rate.\n",
    "\n",
    "    Return value\n",
    "    ------------\n",
    "    Return the precision and the recall of the model\n",
    "    \"\"\"\n",
    "    if regression:\n",
    "        fn, fp, tn, tp = eval_regression(data, data_test, header, target, num_iters, learning_rate)\n",
    "    else:\n",
    "        fn, fp, tn, tp = eval_naive_base(data, data_test, header, target)\n",
    "    return tp / (tp + fp), tp / (tp + fn)\n",
    "\n",
    "def f1_score(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, regression: bool = False, num_iters: int = 1000, learning_rate: float = 0.005) -> float:\n",
    "    \"\"\"\n",
    "    Return the f1 score of the model for a given set of test datas\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        data used to train the regression\n",
    "    data_test: np.ndarray\n",
    "        test dataset\n",
    "    header: np.ndarray\n",
    "        header of the data\n",
    "    target: int\n",
    "        Target column contains label\n",
    "    regression: bool\n",
    "        Indicate if we evaluate the regression or the naive base model\n",
    "    num_iters: int\n",
    "        used to train the regression\n",
    "    learning_rate: float\n",
    "        used to train the regression. Give the learning rate.\n",
    "    \n",
    "    Return value\n",
    "    ------------\n",
    "    Return the f1 score of the model\n",
    "    \"\"\"\n",
    "    if regression:\n",
    "        p, r = precision(data, data_test, header, target, regression, num_iters, learning_rate)\n",
    "    else:\n",
    "        p, r = precision(data, data_test, header, target, regression)\n",
    "    return (2 * p * r) / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(learning_rate)):\n\u001b[0;32m----> 6\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     tmp_2 \u001b[38;5;241m=\u001b[39m f1_score(data, data, header, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m1000\u001b[39m, learning_rate[i])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (tmp \u001b[38;5;241m+\u001b[39m tmp_2) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m>\u001b[39m maximum:\n",
      "Cell \u001b[0;32mIn[27], line 85\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(data, data_test, header, target, regression, num_iters, learning_rate)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mReturn the f1 score of the model for a given set of test datas\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mReturn the f1 score of the model\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m regression:\n\u001b[0;32m---> 85\u001b[0m     p, r \u001b[38;5;241m=\u001b[39m \u001b[43mprecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     p, r \u001b[38;5;241m=\u001b[39m precision(data, data_test, header, target, regression)\n",
      "Cell \u001b[0;32mIn[27], line 54\u001b[0m, in \u001b[0;36mprecision\u001b[0;34m(data, data_test, header, target, regression, num_iters, learning_rate)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mReturn the precision of the model for a given set of test datas\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mReturn the precision and the recall of the model\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m regression:\n\u001b[0;32m---> 54\u001b[0m     fn, fp, tn, tp \u001b[38;5;241m=\u001b[39m \u001b[43meval_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     fn, fp, tn, tp \u001b[38;5;241m=\u001b[39m eval_naive_base(data, data_test, header, target)\n",
      "Cell \u001b[0;32mIn[5], line 203\u001b[0m, in \u001b[0;36meval_regression\u001b[0;34m(data, data_test, header, target, num_iters, learning_rate)\u001b[0m\n\u001b[1;32m    201\u001b[0m TP_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    202\u001b[0m y \u001b[38;5;241m=\u001b[39m data_test[:, target]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m w, b \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m result \u001b[38;5;241m=\u001b[39m evaluate(data_test, target, w, b)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)):\n",
      "Cell \u001b[0;32mIn[5], line 139\u001b[0m, in \u001b[0;36mlogistic_regression\u001b[0;34m(data, target, num_iters, learning_rate)\u001b[0m\n\u001b[1;32m    137\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(X[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    138\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 111\u001b[0m, in \u001b[0;36mtrain_logistic_regression\u001b[0;34m(X, y, w, b, num_iters, learning_rate)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[1;32m    110\u001b[0m     gradient[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m gradient_w(X, y, w, b)\n\u001b[0;32m--> 111\u001b[0m     gradient[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     w \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradient[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    113\u001b[0m     b \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradient[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mgradient_b\u001b[0;34m(X, y, w, b)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    This function calculate the derivate compared to the weights w of the function to minimise, given above\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    return the result of the derivate compared to the weights w of the function to minimise.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (sigma(X \u001b[38;5;241m@\u001b[39m w \u001b[38;5;241m+\u001b[39m b) \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m@\u001b[39m X\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient_b\u001b[39m(X: np\u001b[38;5;241m.\u001b[39mndarray, y: np\u001b[38;5;241m.\u001b[39mndarray, w: np\u001b[38;5;241m.\u001b[39mndarray, b: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    This function calculate the derivate compared to the bias b of the function to minimise, given above\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    return the result of the derivate compared to the bias b of the function to minimise.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(sigma(X \u001b[38;5;241m@\u001b[39m w \u001b[38;5;241m+\u001b[39m b) \u001b[38;5;241m-\u001b[39m y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get optimal parameters w and b for logistic regression\n",
    "maximum = 0\n",
    "index = 0\n",
    "learning_rate = np.linspace(0.001, 1, 1000)\n",
    "for i in range(len(learning_rate)):\n",
    "    tmp = f1_score(data, data_test, header, 3, True, 1000, learning_rate[i])\n",
    "    tmp_2 = f1_score(data, data, header, 3, True, 1000, learning_rate[i])\n",
    "    if (tmp + tmp_2) / 2 > maximum:\n",
    "        maximum = (tmp + tmp_2) / 2\n",
    "        index = i\n",
    "print(learning_rate[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(data, data_test, header, 3, True, 1000, 0.08))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour savoir avec quel `learning rate` je vais travailler, j'ai testé différentes valeurs et j'ai maximisé le f1_score pour le `learning rate` choisi, en prenant la valeur moyenne entre le f1 score obtenu grâce aux données de test et le f1 score obtenu grâce aux données d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`bins` must increase monotonically, when an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     17\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 20\u001b[0m \u001b[43mcompare_methods\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.032\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 15\u001b[0m, in \u001b[0;36mcompare_methods\u001b[0;34m(data, data_test, header, target, num_iters, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComparison between logistic regression and bayes classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1 score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43maccuracy_regression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision_regression\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision_regression\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_score_regression\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/matplotlib/pyplot.py:3224\u001b[0m, in \u001b[0;36mhist\u001b[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[1;32m   3199\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mhist)\n\u001b[1;32m   3200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhist\u001b[39m(\n\u001b[1;32m   3201\u001b[0m     x: ArrayLike \u001b[38;5;241m|\u001b[39m Sequence[ArrayLike],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3222\u001b[0m     BarContainer \u001b[38;5;241m|\u001b[39m Polygon \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[BarContainer \u001b[38;5;241m|\u001b[39m Polygon],\n\u001b[1;32m   3223\u001b[0m ]:\n\u001b[0;32m-> 3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdensity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcumulative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcumulative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhisttype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhisttype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3233\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3234\u001b[0m \u001b[43m        \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstacked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/matplotlib/axes/_axes.py:6859\u001b[0m, in \u001b[0;36mAxes.hist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   6855\u001b[0m \u001b[38;5;66;03m# Loop through datasets\u001b[39;00m\n\u001b[1;32m   6856\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nx):\n\u001b[1;32m   6857\u001b[0m     \u001b[38;5;66;03m# this will automatically overwrite bins,\u001b[39;00m\n\u001b[1;32m   6858\u001b[0m     \u001b[38;5;66;03m# so that each histogram uses the same bins\u001b[39;00m\n\u001b[0;32m-> 6859\u001b[0m     m, bins \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhist_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6860\u001b[0m     tops\u001b[38;5;241m.\u001b[39mappend(m)\n\u001b[1;32m   6861\u001b[0m tops \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tops, \u001b[38;5;28mfloat\u001b[39m)  \u001b[38;5;66;03m# causes problems later if it's an int\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/histograms.py:780\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, density, weights)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m \n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 780\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/histograms.py:431\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    429\u001b[0m     bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(bins)\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(bin_edges[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m bin_edges[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[0;32m--> 431\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    432\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must increase monotonically, when an array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be 1d, when an array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: `bins` must increase monotonically, when an array"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_methods(data: np.ndarray, data_test: np.ndarray, header: np.ndarray, target: int, num_iters: int, learning_rate: float):\n",
    "    accuracy_regression = accuracy(data, data_test, header, target, True, num_iters, learning_rate)\n",
    "    accuracy_bayes = accuracy(data, data_test, header, target, False)\n",
    "\n",
    "    precision_regression = precision(data, data_test, header, target, True, num_iters, learning_rate)\n",
    "    precision_bayes = precision(data, data_test, header, target, False)\n",
    "\n",
    "    f1_score_regression = f1_score(data, data_test, header, target, True, num_iters, learning_rate)\n",
    "    fu_score_bayes = f1_score(data, data_test, header, target, False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Comparison between logistic regression and bayes classifier\")\n",
    "    plt.hist([\"accuracy\", \"precision\", \"recall\", \"f1 score\"], [accuracy_regression, precision_regression[0], precision_regression[1], f1_score_regression])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "compare_methods(data, data_test, header, 3, 1000, 0.032)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
